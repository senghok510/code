{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BbWGD6EK0yG"
      },
      "source": [
        "## Name entity recognition\n",
        "\n",
        "# Author: Hok Seng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrKQHxr0K0yH"
      },
      "source": [
        "<p>\n",
        "    <b>Notebook Sections</b><br>\n",
        "    This notebook is divided into the following sections:\n",
        "    <ul>\n",
        "        <li>A. Setting Environment</li>\n",
        "        <li>B. Set the Devic (cpu or mps) </li>\n",
        "    </ul>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qT8OZbR8K0yH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import requests\n",
        "import logging\n",
        "os.getcwd()\n",
        "from typing import List, Dict, Any,Union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "50Xa3u1CUXHo"
      },
      "outputs": [],
      "source": [
        "from IPython.core.debugger import set_trace\n",
        "import pdb\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "philjBpjK0yJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/onyxia/work/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "# from nltk.tokenize import word_tokenize\n",
        "\n",
        "from itertools import chain\n",
        "import random\n",
        "import copy\n",
        "import itertools\n",
        "\n",
        "\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "# from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix\n",
        "# from sklearn.preprocessing import MultiLabelBinarizer\n",
        "# # from skmultilearn.model_selection import iterative_train_test_split\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import RandomSampler, SequentialSampler\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.optim import AdamW\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForTokenClassification\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.nn import Embedding\n",
        "from torch.nn import LSTM\n",
        "\n",
        "\n",
        "# set the option to display all columns\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydm6H1XhK0yK"
      },
      "source": [
        "### Import clean data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t6YYwSb4K0yK"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('./data/train.csv')\n",
        "valid_df = pd.read_csv('./data/valid.csv')\n",
        "test_df = pd.read_csv('./data/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHtIlrKPK0yK",
        "outputId": "a8e302ac-1c07-4df3-a989-d53f16fc5486"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Dataset size: 34279\n",
            "Valid Data Size: 3808\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train Dataset size: {len(train_df)}\")\n",
        "print(f\"Valid Data Size: {len(valid_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CfdfROUK0yK",
        "outputId": "fe3b081d-4b61-4d0a-bb4c-c7707f6d14ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN Dataset: (34279, 2)\n",
            "VALID Dataset: (3808, 2)\n",
            "TEST Dataset: (9520, 2)\n"
          ]
        }
      ],
      "source": [
        "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
        "print(\"VALID Dataset: {}\".format(valid_df.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_df.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "KTer4xZZK0yL",
        "outputId": "d8a2a797-31c9-4c70-b113-49aa673b24a1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Families of soldiers killed in the conflict jo...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-per,O,O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Police put the number of marchers at 10,000 wh...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The party is divided over Britain 's participa...</td>\n",
              "      <td>O,O,O,O,O,B-gpe,O,O,O,O,B-geo,O,O,O,O,O,O,O,B-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Two Germans and four Nigerian oil workers were...</td>\n",
              "      <td>O,B-gpe,O,O,B-gpe,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The German firm works as a sub-contractor for ...</td>\n",
              "      <td>O,B-gpe,O,O,O,O,O,O,B-org,O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  \\\n",
              "0  Families of soldiers killed in the conflict jo...   \n",
              "1  Police put the number of marchers at 10,000 wh...   \n",
              "2  The party is divided over Britain 's participa...   \n",
              "3  Two Germans and four Nigerian oil workers were...   \n",
              "4  The German firm works as a sub-contractor for ...   \n",
              "\n",
              "                                              labels  \n",
              "0  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-per,O,O,...  \n",
              "1                      O,O,O,O,O,O,O,O,O,O,O,O,O,O,O  \n",
              "2  O,O,O,O,O,B-gpe,O,O,O,O,B-geo,O,O,O,O,O,O,O,B-...  \n",
              "3  O,B-gpe,O,O,B-gpe,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
              "4                        O,B-gpe,O,O,O,O,O,O,B-org,O  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "erabB3xNK0yL",
        "outputId": "15cdf406-30a0-451d-e061-3ff987d5960e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'O,B-gpe,O,O,B-gpe,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-geo,O,O,O,B-geo,O,O'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df[\"labels\"].iloc[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uOULmHgUK0yL",
        "outputId": "2e9135e9-084b-449d-f306-d4e3e0f7476c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Two Germans and four Nigerian oil workers were kidnapped by armed militants during a raid on a boat in Nigeria 's southern oil-rich Delta region .\""
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df[\"sentence\"].iloc[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EMHuOgsWK0yL"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.reset_index(drop=True)\n",
        "valid_df = valid_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0WL8w4fK0yO"
      },
      "source": [
        "### Baseline model\n",
        "### RNN + Softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr1v-rwdK0yO"
      },
      "source": [
        "### Tokenization\n",
        "There are many tokenization methods  including:\n",
        "<li>\n",
        "Bert Tokenizer\n",
        "</li>\n",
        "<li>\n",
        "Byte-Pair Encoding (BPE) - Robust to unknown words\n",
        "</li>\n",
        "\n",
        "### Word tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lkrH_P1RU68O",
        "outputId": "3316d496-7c63-4157-a4da-fb47c93158ef"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Families of soldiers killed in the conflict jo...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-per,O,O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Police put the number of marchers at 10,000 wh...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The party is divided over Britain 's participa...</td>\n",
              "      <td>O,O,O,O,O,B-gpe,O,O,O,O,B-geo,O,O,O,O,O,O,O,B-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Two Germans and four Nigerian oil workers were...</td>\n",
              "      <td>O,B-gpe,O,O,B-gpe,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The German firm works as a sub-contractor for ...</td>\n",
              "      <td>O,B-gpe,O,O,O,O,O,O,B-org,O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  \\\n",
              "0  Families of soldiers killed in the conflict jo...   \n",
              "1  Police put the number of marchers at 10,000 wh...   \n",
              "2  The party is divided over Britain 's participa...   \n",
              "3  Two Germans and four Nigerian oil workers were...   \n",
              "4  The German firm works as a sub-contractor for ...   \n",
              "\n",
              "                                              labels  \n",
              "0  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-per,O,O,...  \n",
              "1                      O,O,O,O,O,O,O,O,O,O,O,O,O,O,O  \n",
              "2  O,O,O,O,O,B-gpe,O,O,O,O,B-geo,O,O,O,O,O,O,O,B-...  \n",
              "3  O,B-gpe,O,O,B-gpe,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
              "4                        O,B-gpe,O,O,O,O,O,O,B-org,O  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "V8av8ki9Stdn"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.normalizers import Lowercase, NFD, StripAccents, Sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RmIe0dsITanq"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vQarD9xXMbM"
      },
      "source": [
        "### Modelling Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272,
          "referenced_widgets": [
            "5945c8126e904191b6ca1312f361cff0",
            "2f06215b6a2647af895382b203feaca0",
            "e1de02d8ac4b45e8b6c35dfc1e71dc90",
            "3813d23c01f14f3b95fce2ca712fd93a",
            "189c69dfdfa2481ab24a59479ff5e97b",
            "09a70faf28404e309f4e3c05f1bb02d8",
            "1f7593918c5e401ab789a1771d909825",
            "e05581d49f71477f84f40892b5c8a745",
            "e4c255c77579465797492281917c352c",
            "65b8dd4c86d2493095b3386b8df7e7ea",
            "b51e3e2196874dd68a98147e50aaf729",
            "ba99a996219b4ef8a9b2cc5430f914be",
            "3a1b0290fdbd486c9a70310f95b9bf34",
            "7f6f39d5105b4e9eb7a239485fdf7fe9",
            "2a192389e7bb4d5193142ffbc4d72b73",
            "272b2672cd684e15ba61975a6c2e4c9c",
            "f0ef094514414301aee9f1a1d8ca4fe4",
            "dd73a726e8ec4e8fa5e39addf6c5af66",
            "53f16e014cc84f9ab2c7a64f2eb507c2",
            "000def3e314f42298493051ff0674214",
            "71da289247ab4c41a926843721a9bfe9",
            "b1d1561ad9e74e5a973fbed659cb87b3",
            "2efbc36a66b346d7a8a69fca6e5f24e9",
            "642267030b7e4e20842d57c075d55f49",
            "864761d672c3454f9b836f2eea1caf8b",
            "5f18b2cfafe844458e7ff0a443690cba",
            "5bb7d1ef4be44d89b242119e853a61c3",
            "aa0398dc830b4455abf474e7248a20a4",
            "dd6d233709414a12b34c11c6e56265ec",
            "e76469bbe36c47ecab85dee37c612b07",
            "9981c20b74064340b1061117a706fec0",
            "92f2e86512b4444d8f5fbd474367043a",
            "c61e8d6ebe904485985037e888a2537b",
            "fef7ee92f3c94dac97c1eb4434a32e8d",
            "924fd220b99f463a9319d2db3e97b6da",
            "1f94123f46274d67ac421f2e062629c0",
            "3f653388b32a4bfaade45629776172b4",
            "509afe3d572f490fac958ed4443aedbe",
            "81d1553105a84128aa82671747901ca0",
            "69ea32dff4484f5ca17225f500e805c9",
            "e0a04f86840542d8b84121ec76c12747",
            "4b013535fc04445d84be0024e99f8510",
            "0b339696365b4b688efe454f02607bbe",
            "47defa877abe48809db99774d04cb8ba"
          ]
        },
        "id": "6VpY9RgrVb0s",
        "outputId": "e03b66f1-394c-4619-aa4b-ce88d187cec8"
      },
      "outputs": [],
      "source": [
        "\n",
        "MAX_LEN = 60\n",
        "TAGSET_SIZE = 10\n",
        "#Batch size\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = 32\n",
        "TEST_BATCH_SIZE = 4\n",
        "#Training epochs\n",
        "EPOCHS = 10\n",
        "#Optimizer para\n",
        "LEARNING_RATE = 1e-03\n",
        "ADAMS_EPSILON = 1e-7\n",
        "#clipping grad norms\n",
        "MAX_GRAD_NORM = 1.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLImVHdFoQkp"
      },
      "source": [
        "### Create Train DataLoader, Valid DataLoader and Test DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnyH5xTlpffe"
      },
      "source": [
        "### We present a novel language representation model known as BERT, which stands for Bidirectional Encoder Representations from Transformers. In contrast to other recent models, BERT is specifically engineered to pre-train deep bidirectional representations from unlabeled text by simultaneously considering both left and right contexts across all layers. Consequently, the pre-trained BERT model can be easily fine-tuned by adding a single output layer, enabling the development of cutting-edge models for various tasks, including question answering and language inference, without the need for significant alterations to the task-specific architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Af85UZFoRR60"
      },
      "outputs": [],
      "source": [
        "# static method\n",
        "def label_to_index(data):\n",
        "  all_labels = data[\"labels\"].apply(lambda x: x.split(','))\n",
        "  all_labels = sorted(set(label for labels in all_labels for label in labels))\n",
        "  label2id = {k:v+1 for v,k in enumerate(all_labels) if k != 'O'}\n",
        "  label2id[\"O\"] = 0\n",
        "\n",
        "  return label2id\n",
        "\n",
        "def index_to_label(data):\n",
        "  all_labels = data[\"labels\"].apply(lambda x:x.split(','))\n",
        "  all_labels = sorted(set(label for labels in all_labels for label in labels))\n",
        "  id2label = {v+1:k for v,k in enumerate(all_labels) if k != 'O'}\n",
        "  id2label[0] = 'O'\n",
        "  return id2label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vorjpYSLTnet",
        "outputId": "fc727df0-4670-4ae9-d803-295241270e50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{1: 'B-geo',\n",
              " 2: 'B-gpe',\n",
              " 3: 'B-org',\n",
              " 4: 'B-per',\n",
              " 5: 'B-tim',\n",
              " 6: 'I-geo',\n",
              " 7: 'I-org',\n",
              " 8: 'I-per',\n",
              " 9: 'I-tim',\n",
              " 0: 'O'}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id2label = index_to_label(train_df)\n",
        "id2label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nuYaP9G0EhoG",
        "outputId": "0e67a2f5-87db-4de7-d88f-41ea6c97ba16"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'B-geo': 1,\n",
              " 'B-gpe': 2,\n",
              " 'B-org': 3,\n",
              " 'B-per': 4,\n",
              " 'B-tim': 5,\n",
              " 'I-geo': 6,\n",
              " 'I-org': 7,\n",
              " 'I-per': 8,\n",
              " 'I-tim': 9,\n",
              " 'O': 0}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label2id = label_to_index(train_df)\n",
        "label2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8e6fe204977042b7b1875dc1285cff08",
            "29f72c857f4a495ab411d01714a17c26",
            "3f91a93bb6614087bb8509981f32030a",
            "d718abdd046d41b2995a2fe0be985f28",
            "a00af3622f5e497da3c3a90d821483c7",
            "858ada749ff54f629903a17624aa90f4",
            "d4c31856b7f2490eb99bdadae3321f08",
            "5e93efa09e6c4c3e9dae5322e3f881c4",
            "0508a08ac8184c02b6cebaac90ac1cd9",
            "6ee51519ddba4bfca279e62d7d8104d8",
            "6fbcc6b31ff4498fa8edf86de7054534"
          ]
        },
        "id": "XhB99Z9S4_TQ",
        "outputId": "2af47bf1-ee72-488b-cd1e-d1cee20f4543"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, BertTokenizer,BertConfig\n",
        "import torch.nn as nn\n",
        "\n",
        "#tokenizer to use\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', clean_up_tokenization_spaces = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ovj_vfkFVbZw"
      },
      "outputs": [],
      "source": [
        "def build_bert_model():\n",
        "    # Load BERT configuration\n",
        "    transformers_pre_trained_model_name = 'bert-base-uncased'\n",
        "\n",
        "    bert_config = BertConfig.from_pretrained(\n",
        "        transformers_pre_trained_model_name,\n",
        "        output_hidden_states=True,  # Get hidden states from BERT\n",
        "        num_labels=TAGSET_SIZE,     # Number of NER tags\n",
        "        max_length=MAX_LEN,        # Maximum sequence length\n",
        "        label2id=label2id,      # Label-to-index mapping\n",
        "        id2label=id2label,      # Index-to-label mapping\n",
        "        gradient_checkpointing=False  # Enable gradient checkpointing (as requested)\n",
        "    )\n",
        "    bert = BertModel.from_pretrained(transformers_pre_trained_model_name, config=bert_config)\n",
        "    return bert\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ccZ-sP9t6CiJ"
      },
      "outputs": [],
      "source": [
        "# absolute positional embedding:\n",
        "# embedding learned during training using sinuisoidal function or cosine function\n",
        "# GELU is an activation function that combine activations using bernoulli distribution - combine RELU and ELU\n",
        "# layer_norm_eps is a coefficient that is used in layer normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "357ggB3hYYhY"
      },
      "outputs": [],
      "source": [
        "bert_model = build_bert_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oU6ReOZ5Ike",
        "outputId": "ff2872fe-12d2-48c4-df32-30348b34d816"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([30522, 768])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert_embeddings = bert_model.embeddings.word_embeddings.weight  # shape: (30522, 768)\n",
        "bert_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH5K5VFwgVGE"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "w4H-wGYEjJ1r"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_retain_labels(sentence, text_labels, tokenizer):\n",
        "  tokenized_sentence = []\n",
        "  labels = []\n",
        "  for word, label in zip(sentence,text_labels):\n",
        "    tokenized_word = tokenizer.tokenize(word)\n",
        "    n_subwords = len(tokenized_word)\n",
        "\n",
        "    #Add the tokenized word to the final tokenized word list\n",
        "    tokenized_sentence.extend(tokenized_word)\n",
        "    labels.extend([label]*n_subwords)\n",
        "\n",
        "  return tokenized_sentence, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c-rxjSggYoX"
      },
      "source": [
        "### Built Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "2gwp0-y2iWkN"
      },
      "outputs": [],
      "source": [
        "class NerDataset_BERT(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        if not isinstance(df, pd.DataFrame):\n",
        "            raise TypeError(\"Input should be a dataframe\")\n",
        "        if \"labels\" not in df.columns or \"sentence\" not in df.columns:\n",
        "            raise ValueError(\"Dataframe should contain labels and sentence columns\")\n",
        "        self.data = df\n",
        "        self.len = len(df)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.data[\"sentence\"][idx].strip().split(' ')\n",
        "        word_labels = self.data['labels'][idx].split(',')\n",
        "\n",
        "        # Tokenize and retain labels\n",
        "        token_sent, token_label = tokenize_and_retain_labels(sentence, word_labels, self.tokenizer)\n",
        "\n",
        "        # Encode with tokenizer\n",
        "        # convert \"sentence\" to \"[CLS] sentence_1 [SEP]\"\n",
        "        encoded_sent_dict = self.tokenizer.encode_plus(\n",
        "            token_sent,\n",
        "            is_split_into_words=False,\n",
        "            add_special_tokens=True,\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = encoded_sent_dict[\"input_ids\"][0]\n",
        "        attn_masks = encoded_sent_dict[\"attention_mask\"][0]\n",
        "\n",
        "        # Initialize labels with -100\n",
        "        label_ids = [-100] * self.max_len\n",
        "        # Align labels, accounting for [CLS] (at index 0)\n",
        "        for i, tok in enumerate(token_label):\n",
        "            if i + 1 < self.max_len - 1:  # Leave space for [SEP]\n",
        "                label_id = label2id.get(tok, -100)\n",
        "                if label_id == -100:\n",
        "                    print(f\"Warning: Invalid label '{tok}' at index {i} in sentence {idx}\")\n",
        "                label_ids[i + 1] = label_id\n",
        "\n",
        "        # Ensure [SEP] and padding tokens have -100\n",
        "        # sep_idx = torch.tensor([len(token_label)+1]) if len(token_label) < self.max_len else torch.tensor([self.max_len])\n",
        "        sep_idx = (input_ids == self.tokenizer.sep_token_id).nonzero(as_tuple=False)[0]\n",
        "\n",
        "        if len(sep_idx) > 0:\n",
        "            label_ids[sep_idx[0].item():] = [-100] * (self.max_len - sep_idx[0].item())\n",
        "\n",
        "        items = {\n",
        "            'input_ids': input_ids,\n",
        "            'attn_masks': attn_masks,\n",
        "            'labels': torch.tensor(label_ids, dtype=torch.long)\n",
        "        }\n",
        "        return items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KbeVCtPMirgD"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_dataset = NerDataset_BERT(train_df,tokenizer,MAX_LEN)\n",
        "valid_dataset = NerDataset_BERT(valid_df, tokenizer,MAX_LEN)\n",
        "test_dataset = NerDataset_BERT(test_df, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RZNoXuCNU_-",
        "outputId": "f84b5e00-d03d-4d0e-925f-6f2d46e453ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Special tokens:\n",
            "CLS token: [CLS] → 101\n",
            "SEP token: [SEP] → 102\n",
            "PAD token: [PAD] → 0\n",
            "MASK token: [MASK] → 103\n",
            "UNK token: [UNK] → 100\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "print(\"Special tokens:\")\n",
        "print(\"CLS token:\", tokenizer.cls_token, \"→\", tokenizer.cls_token_id)\n",
        "print(\"SEP token:\", tokenizer.sep_token, \"→\", tokenizer.sep_token_id)\n",
        "print(\"PAD token:\", tokenizer.pad_token, \"→\", tokenizer.pad_token_id)\n",
        "print(\"MASK token:\", tokenizer.mask_token, \"→\", tokenizer.mask_token_id)\n",
        "print(\"UNK token:\", tokenizer.unk_token, \"→\", tokenizer.unk_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CW_A4jwiyVm",
        "outputId": "eb4d042d-45c0-4a7d-bed6-126b8562fcd5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([ 101, 1996, 2048, 3903, 2031, 5228, 3246, 2012, 4285, 1037, 3521, 3066,\n",
              "         2077, 1057, 1012, 1055, 1012, 2343, 2577, 5747, 3727, 2436, 2220, 2279,\n",
              "         2095, 1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]),\n",
              " 'attn_masks': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'labels': tensor([-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    2,    2,    2,    2,    4,    8,    8,    0,    0,    0,    0,\n",
              "            0,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100])}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[33]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsjr6jzMe6qS"
      },
      "source": [
        "### DataLoader- under the hood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "vFvZRqpILyN7"
      },
      "outputs": [],
      "source": [
        "\n",
        "from typing import Dict, Iterator\n",
        "from math import ceil\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self, dataset, batch_size: int, shuffle: bool = False):\n",
        "        \"\"\"\n",
        "        Custom DataLoader for PyTorch datasets.\n",
        "\n",
        "        Args:\n",
        "            dataset: A PyTorch dataset (e.g., NerDataset_BERT) returning dictionaries with 'input_ids', 'attn_masks', 'labels'.\n",
        "            batch_size (int): Number of samples per batch.\n",
        "            shuffle (bool): Whether to shuffle the dataset before creating batches.\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.n_samples = len(dataset)\n",
        "        self.indices = list(range(self.n_samples))\n",
        "        self._index = 0\n",
        "\n",
        "    def __iter__(self) -> Iterator[Dict[str, torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Initialize iterator and shuffle indices if needed.\n",
        "        \"\"\"\n",
        "        self._index = 0\n",
        "        if self.shuffle:\n",
        "            random.shuffle(self.indices)\n",
        "        return self\n",
        "\n",
        "    def __next__(self) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Return the next batch of data.\n",
        "        \"\"\"\n",
        "        if self._index >= ceil(self.n_samples / self.batch_size):\n",
        "            raise StopIteration\n",
        "\n",
        "        start_idx = self._index * self.batch_size\n",
        "        end_idx = min(start_idx + self.batch_size, self.n_samples)\n",
        "        batch_indices = self.indices[start_idx:end_idx]\n",
        "\n",
        "        batch = [self.dataset[idx] for idx in batch_indices]\n",
        "\n",
        "        #collate batch: Stack tensors for each key\n",
        "        batch_dict = {\n",
        "            'input_ids': torch.stack([sample['input_ids'] for sample in batch]),\n",
        "            'attn_masks': torch.stack([sample['attn_masks'] for sample in batch]),\n",
        "            'labels': torch.stack([sample['labels'] for sample in batch])\n",
        "        }\n",
        "\n",
        "        self._index += 1\n",
        "        return batch_dict\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Return the number of batches.\n",
        "        \"\"\"\n",
        "        return ceil(self.n_samples / self.batch_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "tnZ6xOujq9Ad"
      },
      "outputs": [],
      "source": [
        "train_params = {\n",
        "    'batch_size': TRAIN_BATCH_SIZE,\n",
        "    'shuffle': True\n",
        "}\n",
        "valid_params = {\n",
        "    'batch_size': VALID_BATCH_SIZE,\n",
        "    'shuffle':False\n",
        "}\n",
        "test_params = {\n",
        "    'batch_size': TEST_BATCH_SIZE,\n",
        "    'shuffle': False\n",
        "}\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, **train_params)\n",
        "valid_dataloader = DataLoader(valid_dataset, **valid_params)\n",
        "test_dataloader = DataLoader(test_dataset, **test_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMXTJMcmJ3uD",
        "outputId": "4d54c99b-2c36-4182-9632-c65e14862b64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch shapes:\n",
            "input_ids: torch.Size([32, 60])\n",
            "attn_masks: torch.Size([32, 60])\n",
            "labels: torch.Size([32, 60])\n"
          ]
        }
      ],
      "source": [
        "# Test the DataLoader\n",
        "try:\n",
        "    for batch in train_dataloader:\n",
        "        print(\"Batch shapes:\")\n",
        "        print(f\"input_ids: {batch['input_ids'].shape}\")\n",
        "        print(f\"attn_masks: {batch['attn_masks'].shape}\")\n",
        "        print(f\"labels: {batch['labels'].shape}\")\n",
        "        break\n",
        "except Exception as e:\n",
        "    print(f\"Error in DataLoader: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6mWg1XREfoD",
        "outputId": "c2be948c-8aa9-4504-c1a3-3b5005c72ec2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "34304"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "1072*32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Jwxgfd5MZ76D",
        "outputId": "415b03d1-fcf8-451f-ad3c-49cef726752b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"architectures\": [\n",
              "    \"BertForMaskedLM\"\n",
              "  ],\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"O\",\n",
              "    \"1\": \"B-geo\",\n",
              "    \"2\": \"B-gpe\",\n",
              "    \"3\": \"B-org\",\n",
              "    \"4\": \"B-per\",\n",
              "    \"5\": \"B-tim\",\n",
              "    \"6\": \"I-geo\",\n",
              "    \"7\": \"I-org\",\n",
              "    \"8\": \"I-per\",\n",
              "    \"9\": \"I-tim\"\n",
              "  },\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"label2id\": {\n",
              "    \"B-geo\": 1,\n",
              "    \"B-gpe\": 2,\n",
              "    \"B-org\": 3,\n",
              "    \"B-per\": 4,\n",
              "    \"B-tim\": 5,\n",
              "    \"I-geo\": 6,\n",
              "    \"I-org\": 7,\n",
              "    \"I-per\": 8,\n",
              "    \"I-tim\": 9,\n",
              "    \"O\": 0\n",
              "  },\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_length\": 60,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"output_hidden_states\": true,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"torch_dtype\": \"float32\",\n",
              "  \"transformers_version\": \"4.54.1\",\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config = bert_model.config\n",
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "RFPpFDDqj42S"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience: int = 3, mode: str = 'max'):\n",
        "        self.patience = patience\n",
        "        self.mode = mode\n",
        "        self.best_score = -float('inf') if mode == 'max' else float('inf')\n",
        "        self.best_model_state = None\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, score: float, model: torch.nn.Module):\n",
        "        if self.mode == 'max' and score > self.best_score:\n",
        "            self.best_score = score\n",
        "            self.best_model_state = model.state_dict()\n",
        "            self.counter = 0\n",
        "        elif self.mode == 'min' and score < self.best_score:\n",
        "            self.best_score = score\n",
        "            self.best_model_state = model.state_dict()\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "    def load_best_model(self, model: torch.nn.Module):\n",
        "        if self.best_model_state is not None:\n",
        "            model.load_state_dict(self.best_model_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "oSGHGd9nfUYw"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(pred_tags: torch.Tensor, labels: torch.Tensor, mask: torch.Tensor) -> float:\n",
        "    \"\"\"\n",
        "    Compute accuracy for a batch, excluding ignored labels (-100) and special tokens.\n",
        "\n",
        "    Args:\n",
        "        pred_tags (torch.Tensor): Predicted tag indices, shape (batch_size, seq_len).\n",
        "        labels (torch.Tensor): True label indices, shape (batch_size, seq_len).\n",
        "        mask (torch.Tensor): Attention mask, shape (batch_size, seq_len).\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy for the batch.\n",
        "    \"\"\"\n",
        "    valid_mask = (mask == 1) & (labels != -100)\n",
        "    correct = (pred_tags == labels) & valid_mask\n",
        "    correct_count = correct.sum().item()\n",
        "    total_count = valid_mask.sum().item()\n",
        "    return correct_count / total_count if total_count > 0 else 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/onyxia/work/bi-lstm-crf/bi_lstm_crf/model/crf.py\n"
          ]
        }
      ],
      "source": [
        "import inspect\n",
        "from bi_lstm_crf.model.crf import CRF\n",
        "print(inspect.getfile(CRF))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/onyxia/work'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "WRX9fFWGZmEZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from typing import Optional\n",
        "\n",
        "class BERT_Bi_CRF(nn.Module):\n",
        "    def __init__(self, bert, tagset_size: int, hidden_dim: int = 128, num_layers: int = 1, dropout: float = 0.3):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        embedding_dim = self.bert.config.hidden_size\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.crf = CRF(hidden_dim * 2, tagset_size)\n",
        "\n",
        "    def build_features(self, input_ids, attention_mask):\n",
        "        # Get contextual embeddings from BERT\n",
        "        with torch.no_grad():\n",
        "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        embeds = outputs.last_hidden_state  # shape: (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        # Compute mask (padding = 0)\n",
        "        masks = attention_mask.bool()\n",
        "\n",
        "        # Sequence lengths needed for packing\n",
        "        seq_length = masks.sum(1)\n",
        "        sorted_seq_length, perm_idx = seq_length.sort(descending=True)\n",
        "        embeds = embeds[perm_idx]\n",
        "        pack_sequence = pack_padded_sequence(embeds, lengths=sorted_seq_length.cpu(), batch_first=True, enforce_sorted=True)\n",
        "        packed_output, _ = self.lstm(pack_sequence)\n",
        "        lstm_out, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "        _, unperm_idx = perm_idx.sort()\n",
        "        lstm_out = lstm_out[unperm_idx]\n",
        "        masks = masks[unperm_idx]\n",
        "\n",
        "        return lstm_out, masks\n",
        "\n",
        "    def loss(self, input_ids, attention_mask, tags):\n",
        "        features, masks = self.build_features(input_ids, attention_mask)\n",
        "        return self.crf.loss(features, tags, masks)\n",
        "\n",
        "    def forward(self, input_ids: torch.LongTensor, attention_mask: torch.LongTensor) -> torch.Tensor:\n",
        "        features, masks = self.build_features(input_ids, attention_mask)\n",
        "        scores, tag_seq = self.crf(features, masks)\n",
        "        return scores, tag_seq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = BERT_Bi_CRF(bert_model, tagset_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BERT_Bi_CRF(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (lstm): LSTM(768, 128, batch_first=True, bidirectional=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (crf): CRF(\n",
              "    (fc): Linear(in_features=256, out_features=12, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Optimizer\n",
        "optimizer = Adam(model.parameters(), lr=3e-5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loss = []\n",
        "valid_loss = []\n",
        "accuracy_valid= []\n",
        "f1_valid = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqoAAAGyCAYAAAAs6OYBAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHftJREFUeJzt3W9s3VX9wPFP29FbCLQM59ptFiYoovzZYGO1/AnBVJtAhntgrGC2ufBHZBJco7IxWEVgnQhkCRQXJogPwE0JEOOWIlYXg9QsbGuCskFgwCaxZVNpZ9GWtd/fA0P9lXW4W9rusL5eyX2w4zn3e66H6ptv770ryLIsCwAASEzh4d4AAAAMRagCAJAkoQoAQJKEKgAASRKqAAAkSagCAJAkoQoAQJKEKgAASRKqAAAkSagCAJCkvEP197//fcydOzemTp0aBQUF8eSTT/7PNZs2bYpzzjkncrlcfOITn4iHH354GFsFAGA8yTtUu7u7Y8aMGdHU1HRI81999dW49NJL4+KLL462trb41re+FVdddVU89dRTeW8WAIDxoyDLsmzYiwsK4oknnoh58+YddM6NN94YGzZsiD/96U8DY1/5ylfirbfeiubm5uFeGgCAI9yE0b5Aa2tr1NTUDBqrra2Nb33rWwdd09PTEz09PQN/7u/vj7///e/xkY98JAoKCkZrqwAADFOWZbFv376YOnVqFBaOzMegRj1U29vbo7y8fNBYeXl5dHV1xb/+9a84+uijD1jT2NgYt95662hvDQCAEbZ79+742Mc+NiLPNeqhOhzLli2L+vr6gT93dnbGiSeeGLt3747S0tLDuDMAAIbS1dUVlZWVcdxxx43Yc456qFZUVERHR8egsY6OjigtLR3ybmpERC6Xi1wud8B4aWmpUAUASNhIvk1z1L9Htbq6OlpaWgaNPf3001FdXT3alwYA4EMs71D95z//GW1tbdHW1hYR//n6qba2tti1a1dE/OfX9gsWLBiYf+2118bOnTvju9/9buzYsSPuv//++PnPfx5LliwZmVcAAMARKe9Qfe655+Lss8+Os88+OyIi6uvr4+yzz44VK1ZERMRf//rXgWiNiPj4xz8eGzZsiKeffjpmzJgRd999d/z4xz+O2traEXoJAAAciT7Q96iOla6urigrK4vOzk7vUQUASNBo9Nqov0cVAACGQ6gCAJAkoQoAQJKEKgAASRKqAAAkSagCAJAkoQoAQJKEKgAASRKqAAAkSagCAJAkoQoAQJKEKgAASRKqAAAkSagCAJAkoQoAQJKEKgAASRKqAAAkSagCAJAkoQoAQJKEKgAASRKqAAAkSagCAJAkoQoAQJKEKgAASRKqAAAkSagCAJAkoQoAQJKEKgAASRKqAAAkSagCAJAkoQoAQJKEKgAASRKqAAAkSagCAJAkoQoAQJKEKgAASRKqAAAkSagCAJAkoQoAQJKEKgAASRKqAAAkSagCAJAkoQoAQJKEKgAASRKqAAAkSagCAJAkoQoAQJKEKgAASRKqAAAkSagCAJAkoQoAQJKEKgAASRKqAAAkSagCAJAkoQoAQJKEKgAASRKqAAAkSagCAJAkoQoAQJKEKgAASRKqAAAkSagCAJAkoQoAQJKEKgAASRKqAAAkSagCAJCkYYVqU1NTTJ8+PUpKSqKqqio2b978vvNXr14dn/rUp+Loo4+OysrKWLJkSfz73/8e1oYBABgf8g7V9evXR319fTQ0NMTWrVtjxowZUVtbG2+++eaQ8x999NFYunRpNDQ0xPbt2+PBBx+M9evXx0033fSBNw8AwJEr71C955574uqrr45FixbFZz7zmVizZk0cc8wx8dBDDw05/9lnn43zzz8/rrjiipg+fXp84QtfiMsvv/x/3oUFAGB8yytUe3t7Y8uWLVFTU/PfJygsjJqammhtbR1yzXnnnRdbtmwZCNOdO3fGxo0b45JLLjnodXp6eqKrq2vQAwCA8WVCPpP37t0bfX19UV5ePmi8vLw8duzYMeSaK664Ivbu3RsXXHBBZFkW+/fvj2uvvfZ9f/Xf2NgYt956az5bAwDgCDPqn/rftGlTrFy5Mu6///7YunVrPP7447Fhw4a47bbbDrpm2bJl0dnZOfDYvXv3aG8TAIDE5HVHddKkSVFUVBQdHR2Dxjs6OqKiomLINbfcckvMnz8/rrrqqoiIOPPMM6O7uzuuueaaWL58eRQWHtjKuVwucrlcPlsDAOAIk9cd1eLi4pg1a1a0tLQMjPX390dLS0tUV1cPuebtt98+IEaLiooiIiLLsnz3CwDAOJHXHdWIiPr6+li4cGHMnj075syZE6tXr47u7u5YtGhRREQsWLAgpk2bFo2NjRERMXfu3Ljnnnvi7LPPjqqqqnj55Zfjlltuiblz5w4EKwAAvFfeoVpXVxd79uyJFStWRHt7e8ycOTOam5sHPmC1a9euQXdQb7755igoKIibb7453njjjfjoRz8ac+fOjTvuuGPkXgUAAEecguxD8Pv3rq6uKCsri87OzigtLT3c2wEA4D1Go9dG/VP/AAAwHEIVAIAkCVUAAJIkVAEASJJQBQAgSUIVAIAkCVUAAJIkVAEASJJQBQAgSUIVAIAkCVUAAJIkVAEASJJQBQAgSUIVAIAkCVUAAJIkVAEASJJQBQAgSUIVAIAkCVUAAJIkVAEASJJQBQAgSUIVAIAkCVUAAJIkVAEASJJQBQAgSUIVAIAkCVUAAJIkVAEASJJQBQAgSUIVAIAkCVUAAJIkVAEASJJQBQAgSUIVAIAkCVUAAJIkVAEASJJQBQAgSUIVAIAkCVUAAJIkVAEASJJQBQAgSUIVAIAkCVUAAJIkVAEASJJQBQAgSUIVAIAkCVUAAJIkVAEASJJQBQAgSUIVAIAkCVUAAJIkVAEASJJQBQAgSUIVAIAkCVUAAJIkVAEASJJQBQAgSUIVAIAkCVUAAJIkVAEASJJQBQAgSUIVAIAkCVUAAJIkVAEASJJQBQAgSUIVAIAkDStUm5qaYvr06VFSUhJVVVWxefPm953/1ltvxeLFi2PKlCmRy+Xi1FNPjY0bNw5rwwAAjA8T8l2wfv36qK+vjzVr1kRVVVWsXr06amtr48UXX4zJkycfML+3tzc+//nPx+TJk+Oxxx6LadOmxeuvvx7HH3/8SOwfAIAjVEGWZVk+C6qqquLcc8+N++67LyIi+vv7o7KyMq6//vpYunTpAfPXrFkTP/zhD2PHjh1x1FFHDWuTXV1dUVZWFp2dnVFaWjqs5wAAYPSMRq/l9av/3t7e2LJlS9TU1Pz3CQoLo6amJlpbW4dc88tf/jKqq6tj8eLFUV5eHmeccUasXLky+vr6Dnqdnp6e6OrqGvQAAGB8yStU9+7dG319fVFeXj5ovLy8PNrb24dcs3Pnznjssceir68vNm7cGLfcckvcfffdcfvttx/0Oo2NjVFWVjbwqKyszGebAAAcAUb9U//9/f0xefLkeOCBB2LWrFlRV1cXy5cvjzVr1hx0zbJly6Kzs3PgsXv37tHeJgAAicnrw1STJk2KoqKi6OjoGDTe0dERFRUVQ66ZMmVKHHXUUVFUVDQw9ulPfzra29ujt7c3iouLD1iTy+Uil8vlszUAAI4wed1RLS4ujlmzZkVLS8vAWH9/f7S0tER1dfWQa84///x4+eWXo7+/f2DspZdeiilTpgwZqQAAEDGMX/3X19fH2rVr46c//Wls3749vvGNb0R3d3csWrQoIiIWLFgQy5YtG5j/jW98I/7+97/HDTfcEC+99FJs2LAhVq5cGYsXLx65VwEAwBEn7+9Rrauriz179sSKFSuivb09Zs6cGc3NzQMfsNq1a1cUFv63fysrK+Opp56KJUuWxFlnnRXTpk2LG264IW688caRexUAABxx8v4e1cPB96gCAKTtsH+PKgAAjBWhCgBAkoQqAABJEqoAACRJqAIAkCShCgBAkoQqAABJEqoAACRJqAIAkCShCgBAkoQqAABJEqoAACRJqAIAkCShCgBAkoQqAABJEqoAACRJqAIAkCShCgBAkoQqAABJEqoAACRJqAIAkCShCgBAkoQqAABJEqoAACRJqAIAkCShCgBAkoQqAABJEqoAACRJqAIAkCShCgBAkoQqAABJEqoAACRJqAIAkCShCgBAkoQqAABJEqoAACRJqAIAkCShCgBAkoQqAABJEqoAACRJqAIAkCShCgBAkoQqAABJEqoAACRJqAIAkCShCgBAkoQqAABJEqoAACRJqAIAkCShCgBAkoQqAABJEqoAACRJqAIAkCShCgBAkoQqAABJEqoAACRJqAIAkCShCgBAkoQqAABJEqoAACRJqAIAkCShCgBAkoQqAABJEqoAACRJqAIAkCShCgBAkoYVqk1NTTF9+vQoKSmJqqqq2Lx58yGtW7duXRQUFMS8efOGc1kAAMaRvEN1/fr1UV9fHw0NDbF169aYMWNG1NbWxptvvvm+61577bX49re/HRdeeOGwNwsAwPiRd6jec889cfXVV8eiRYviM5/5TKxZsyaOOeaYeOihhw66pq+vL7761a/GrbfeGieffPIH2jAAAONDXqHa29sbW7ZsiZqamv8+QWFh1NTURGtr60HXff/734/JkyfHlVdeeUjX6enpia6urkEPAADGl7xCde/evdHX1xfl5eWDxsvLy6O9vX3INc8880w8+OCDsXbt2kO+TmNjY5SVlQ08Kisr89kmAABHgFH91P++ffti/vz5sXbt2pg0adIhr1u2bFl0dnYOPHbv3j2KuwQAIEUT8pk8adKkKCoqio6OjkHjHR0dUVFRccD8V155JV577bWYO3fuwFh/f/9/LjxhQrz44otxyimnHLAul8tFLpfLZ2sAABxh8rqjWlxcHLNmzYqWlpaBsf7+/mhpaYnq6uoD5p922mnx/PPPR1tb28Djsssui4svvjja2tr8Sh8AgIPK645qRER9fX0sXLgwZs+eHXPmzInVq1dHd3d3LFq0KCIiFixYENOmTYvGxsYoKSmJM844Y9D6448/PiLigHEAAPj/8g7Vurq62LNnT6xYsSLa29tj5syZ0dzcPPABq127dkVhob/wCgCAD6Ygy7LscG/if+nq6oqysrLo7OyM0tLSw70dAADeYzR6za1PAACSJFQBAEiSUAUAIElCFQCAJAlVAACSJFQBAEiSUAUAIElCFQCAJAlVAACSJFQBAEiSUAUAIElCFQCAJAlVAACSJFQBAEiSUAUAIElCFQCAJAlVAACSJFQBAEiSUAUAIElCFQCAJAlVAACSJFQBAEiSUAUAIElCFQCAJAlVAACSJFQBAEiSUAUAIElCFQCAJAlVAACSJFQBAEiSUAUAIElCFQCAJAlVAACSJFQBAEiSUAUAIElCFQCAJAlVAACSJFQBAEiSUAUAIElCFQCAJAlVAACSJFQBAEiSUAUAIElCFQCAJAlVAACSJFQBAEiSUAUAIElCFQCAJAlVAACSJFQBAEiSUAUAIElCFQCAJAlVAACSJFQBAEiSUAUAIElCFQCAJAlVAACSJFQBAEiSUAUAIElCFQCAJAlVAACSJFQBAEiSUAUAIElCFQCAJAlVAACSJFQBAEjSsEK1qakppk+fHiUlJVFVVRWbN28+6Ny1a9fGhRdeGBMnToyJEydGTU3N+84HAICIYYTq+vXro76+PhoaGmLr1q0xY8aMqK2tjTfffHPI+Zs2bYrLL788fve730Vra2tUVlbGF77whXjjjTc+8OYBADhyFWRZluWzoKqqKs4999y47777IiKiv78/Kisr4/rrr4+lS5f+z/V9fX0xceLEuO+++2LBggWHdM2urq4oKyuLzs7OKC0tzWe7AACMgdHotbzuqPb29saWLVuipqbmv09QWBg1NTXR2tp6SM/x9ttvxzvvvBMnnHDCQef09PREV1fXoAcAAONLXqG6d+/e6Ovri/Ly8kHj5eXl0d7efkjPceONN8bUqVMHxe57NTY2RllZ2cCjsrIyn20CAHAEGNNP/a9atSrWrVsXTzzxRJSUlBx03rJly6Kzs3PgsXv37jHcJQAAKZiQz+RJkyZFUVFRdHR0DBrv6OiIioqK91171113xapVq+I3v/lNnHXWWe87N5fLRS6Xy2drAAAcYfK6o1pcXByzZs2KlpaWgbH+/v5oaWmJ6urqg667884747bbbovm5uaYPXv28HcLAMC4kdcd1YiI+vr6WLhwYcyePTvmzJkTq1evju7u7li0aFFERCxYsCCmTZsWjY2NERHxgx/8IFasWBGPPvpoTJ8+feC9rMcee2wce+yxI/hSAAA4kuQdqnV1dbFnz55YsWJFtLe3x8yZM6O5uXngA1a7du2KwsL/3qj90Y9+FL29vfGlL31p0PM0NDTE9773vQ+2ewAAjlh5f4/q4eB7VAEA0nbYv0cVAADGilAFACBJQhUAgCQJVQAAkiRUAQBIklAFACBJQhUAgCQJVQAAkiRUAQBIklAFACBJQhUAgCQJVQAAkiRUAQBIklAFACBJQhUAgCQJVQAAkiRUAQBIklAFACBJQhUAgCQJVQAAkiRUAQBIklAFACBJQhUAgCQJVQAAkiRUAQBIklAFACBJQhUAgCQJVQAAkiRUAQBIklAFACBJQhUAgCQJVQAAkiRUAQBIklAFACBJQhUAgCQJVQAAkiRUAQBIklAFACBJQhUAgCQJVQAAkiRUAQBIklAFACBJQhUAgCQJVQAAkiRUAQBIklAFACBJQhUAgCQJVQAAkiRUAQBIklAFACBJQhUAgCQJVQAAkiRUAQBIklAFACBJQhUAgCQJVQAAkiRUAQBIklAFACBJQhUAgCQJVQAAkiRUAQBIklAFACBJQhUAgCQJVQAAkiRUAQBIklAFACBJwwrVpqammD59epSUlERVVVVs3rz5fef/4he/iNNOOy1KSkrizDPPjI0bNw5rswAAjB95h+r69eujvr4+GhoaYuvWrTFjxoyora2NN998c8j5zz77bFx++eVx5ZVXxrZt22LevHkxb968+NOf/vSBNw8AwJGrIMuyLJ8FVVVVce6558Z9990XERH9/f1RWVkZ119/fSxduvSA+XV1ddHd3R2/+tWvBsY++9nPxsyZM2PNmjWHdM2urq4oKyuLzs7OKC0tzWe7AACMgdHotQn5TO7t7Y0tW7bEsmXLBsYKCwujpqYmWltbh1zT2toa9fX1g8Zqa2vjySefPOh1enp6oqenZ+DPnZ2dEfGf/wIAAEjPu52W5z3Q95VXqO7duzf6+vqivLx80Hh5eXns2LFjyDXt7e1Dzm9vbz/odRobG+PWW289YLyysjKf7QIAMMb+9re/RVlZ2Yg8V16hOlaWLVs26C7sW2+9FSeddFLs2rVrxF446erq6orKysrYvXu3t3qMA857fHHe44vzHl86OzvjxBNPjBNOOGHEnjOvUJ00aVIUFRVFR0fHoPGOjo6oqKgYck1FRUVe8yMicrlc5HK5A8bLysr8gz6OlJaWOu9xxHmPL857fHHe40th4ch9+2lez1RcXByzZs2KlpaWgbH+/v5oaWmJ6urqIddUV1cPmh8R8fTTTx90PgAARAzjV//19fWxcOHCmD17dsyZMydWr14d3d3dsWjRooiIWLBgQUybNi0aGxsjIuKGG26Iiy66KO6+++649NJLY926dfHcc8/FAw88MLKvBACAI0reoVpXVxd79uyJFStWRHt7e8ycOTOam5sHPjC1a9euQbd8zzvvvHj00Ufj5ptvjptuuik++clPxpNPPhlnnHHGIV8zl8tFQ0PDkG8H4MjjvMcX5z2+OO/xxXmPL6Nx3nl/jyoAAIyFkXu3KwAAjCChCgBAkoQqAABJEqoAACQpmVBtamqK6dOnR0lJSVRVVcXmzZvfd/4vfvGLOO2006KkpCTOPPPM2Lhx4xjtlJGQz3mvXbs2Lrzwwpg4cWJMnDgxampq/uc/H6Ql35/vd61bty4KCgpi3rx5o7tBRlS+5/3WW2/F4sWLY8qUKZHL5eLUU0/1v+kfIvme9+rVq+NTn/pUHH300VFZWRlLliyJf//732O0W4br97//fcydOzemTp0aBQUF8eSTT/7PNZs2bYpzzjkncrlcfOITn4iHH344/wtnCVi3bl1WXFycPfTQQ9mf//zn7Oqrr86OP/74rKOjY8j5f/jDH7KioqLszjvvzF544YXs5ptvzo466qjs+eefH+OdMxz5nvcVV1yRNTU1Zdu2bcu2b9+efe1rX8vKysqyv/zlL2O8c4Yj3/N+16uvvppNmzYtu/DCC7MvfvGLY7NZPrB8z7unpyebPXt2dskll2TPPPNM9uqrr2abNm3K2traxnjnDEe+5/3II49kuVwue+SRR7JXX301e+qpp7IpU6ZkS5YsGeOdk6+NGzdmy5cvzx5//PEsIrInnnjifefv3LkzO+aYY7L6+vrshRdeyO69996sqKgoa25uzuu6SYTqnDlzssWLFw/8ua+vL5s6dWrW2Ng45Pwvf/nL2aWXXjporKqqKvv6178+qvtkZOR73u+1f//+7Ljjjst++tOfjtYWGUHDOe/9+/dn5513XvbjH/84W7hwoVD9EMn3vH/0ox9lJ598ctbb2ztWW2QE5Xveixcvzj73uc8NGquvr8/OP//8Ud0nI+tQQvW73/1udvrppw8aq6ury2pra/O61mH/1X9vb29s2bIlampqBsYKCwujpqYmWltbh1zT2to6aH5ERG1t7UHnk47hnPd7vf322/HOO+/ECSecMFrbZIQM97y///3vx+TJk+PKK68ci20yQoZz3r/85S+juro6Fi9eHOXl5XHGGWfEypUro6+vb6y2zTAN57zPO++82LJly8DbA3bu3BkbN26MSy65ZEz2zNgZqVbL+2+mGml79+6Nvr6+gb/Z6l3l5eWxY8eOIde0t7cPOb+9vX3U9snIGM55v9eNN94YU6dOPeAHgPQM57yfeeaZePDBB6OtrW0MdshIGs5579y5M37729/GV7/61di4cWO8/PLLcd1118U777wTDQ0NY7Fthmk4533FFVfE3r1744ILLogsy2L//v1x7bXXxk033TQWW2YMHazVurq64l//+lccffTRh/Q8h/2OKuRj1apVsW7dunjiiSeipKTkcG+HEbZv376YP39+rF27NiZNmnS4t8MY6O/vj8mTJ8cDDzwQs2bNirq6uli+fHmsWbPmcG+NUbBp06ZYuXJl3H///bF169Z4/PHHY8OGDXHbbbcd7q2RqMN+R3XSpElRVFQUHR0dg8Y7OjqioqJiyDUVFRV5zScdwznvd911112xatWq+M1vfhNnnXXWaG6TEZLveb/yyivx2muvxdy5cwfG+vv7IyJiwoQJ8eKLL8Ypp5wyuptm2Ibz8z1lypQ46qijoqioaGDs05/+dLS3t0dvb28UFxeP6p4ZvuGc9y233BLz58+Pq666KiIizjzzzOju7o5rrrkmli9fHoWF7p8dKQ7WaqWlpYd8NzUigTuqxcXFMWvWrGhpaRkY6+/vj5aWlqiurh5yTXV19aD5ERFPP/30QeeTjuGcd0TEnXfeGbfddls0NzfH7Nmzx2KrjIB8z/u0006L559/Ptra2gYel112WVx88cXR1tYWlZWVY7l98jScn+/zzz8/Xn755YF/IYmIeOmll2LKlCkiNXHDOe+33377gBh9919S/vMZHY4UI9Zq+X3Oa3SsW7cuy+Vy2cMPP5y98MIL2TXXXJMdf/zxWXt7e5ZlWTZ//vxs6dKlA/P/8Ic/ZBMmTMjuuuuubPv27VlDQ4Ovp/oQyfe8V61alRUXF2ePPfZY9te//nXgsW/fvsP1EshDvuf9Xj71/+GS73nv2rUrO+6447JvfvOb2Ysvvpj96le/yiZPnpzdfvvth+slkId8z7uhoSE77rjjsp/97GfZzp07s1//+tfZKaeckn35y18+XC+BQ7Rv375s27Zt2bZt27KIyO65555s27Zt2euvv55lWZYtXbo0mz9//sD8d7+e6jvf+U62ffv2rKmp6cP79VRZlmX33ntvduKJJ2bFxcXZnDlzsj/+8Y8D/9lFF12ULVy4cND8n//859mpp56aFRcXZ6effnq2YcOGMd4xH0Q+533SSSdlEXHAo6GhYew3zrDk+/P9/wnVD598z/vZZ5/Nqqqqslwul5188snZHXfcke3fv3+Md81w5XPe77zzTva9730vO+WUU7KSkpKssrIyu+6667J//OMfY79x8vK73/1uyP8vfvd8Fy5cmF100UUHrJk5c2ZWXFycnXzyydlPfvKTvK9bkGXutQMAkJ7D/h5VAAAYilAFACBJQhUAgCQJVQAAkiRUAQBIklAFACBJQhUAgCQJVQAAkiRUAQBIklAFACBJQhUAgCQJVQAAkvR/kB9t1Nd2bicAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "# Set up interactive plot\n",
        "plt.ion()\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "train_loss_history = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 6226, 1997,  ...,    0,    0,    0],\n",
              "         [ 101, 2859, 4247,  ...,    0,    0,    0],\n",
              "         [ 101, 2028, 2351,  ...,    0,    0,    0],\n",
              "         ...,\n",
              "         [ 101, 2720, 1012,  ...,    0,    0,    0],\n",
              "         [ 101, 2634, 2038,  ...,    0,    0,    0],\n",
              "         [ 101, 2009, 2435,  ...,    0,    0,    0]]),\n",
              " 'attn_masks': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
              " 'labels': tensor([[-100,    0,    0,  ..., -100, -100, -100],\n",
              "         [-100,    1,    0,  ..., -100, -100, -100],\n",
              "         [-100,    0,    0,  ..., -100, -100, -100],\n",
              "         ...,\n",
              "         [-100,    4,    4,  ..., -100, -100, -100],\n",
              "         [-100,    1,    0,  ..., -100, -100, -100],\n",
              "         [-100,    0,    0,  ..., -100, -100, -100]])}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(train_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "num_epochs = 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(\"=\"*50)\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    batch_losses = []\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_dataloader, start=1):\n",
        "        print(f\"Batch {batch_idx}/{len(train_dataloader)}\")\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attn_masks = batch['attn_masks'].to(device)\n",
        "        labels = batch['labels'].clone()\n",
        "        labels[labels == -100] = 0\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = model.loss(input_ids, attn_masks, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        batch_losses.append(batch_loss)\n",
        "        print(f\"Batch loss: {batch_loss:.4f}\")\n",
        "\n",
        "    avg_train_loss = sum(batch_losses) / len(batch_losses)\n",
        "    train_loss_history.append(avg_train_loss)\n",
        "    print(f\" Train loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # === Update plot ===\n",
        "    clear_output(wait=True)\n",
        "    ax.clear()\n",
        "    ax.plot(train_loss_history, label='Train Loss', marker='o')\n",
        "    ax.set_title(\"Training Loss Over Epochs\")\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"Loss\")\n",
        "    ax.grid(True)\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.pause(0.1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"bert_bilstm_crf_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "}, \"checkpoint.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(\"bert_bilstm_crf_model.pt\", map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = BERT_Bi_CRF(bert_model, tagset_size=10).to(device)\n",
        "model.load_state_dict(torch.load(\"bert_bilstm_crf_model.pt\", map_location=device))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 17, filtered true_tags len: 14\n",
            "Batch 1/119\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/onyxia/work/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 46, filtered true_tags len: 30\n",
            "Batch 2/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 19, filtered true_tags len: 39\n",
            "Batch 3/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 15, filtered true_tags len: 9\n",
            "Batch 4/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 30, filtered true_tags len: 21\n",
            "Batch 5/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 26, filtered true_tags len: 21\n",
            "Batch 6/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 28, filtered true_tags len: 27\n",
            "Batch 7/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 28, filtered true_tags len: 17\n",
            "Batch 8/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 18, filtered true_tags len: 20\n",
            "Batch 9/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 44, filtered true_tags len: 11\n",
            "Batch 10/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 33, filtered true_tags len: 19\n",
            "Batch 11/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 49, filtered true_tags len: 42\n",
            "Batch 12/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 28, filtered true_tags len: 38\n",
            "Batch 13/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 26, filtered true_tags len: 24\n",
            "Batch 14/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 24, filtered true_tags len: 18\n",
            "Batch 15/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 13, filtered true_tags len: 26\n",
            "Batch 16/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 30, filtered true_tags len: 15\n",
            "Batch 17/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 15, filtered true_tags len: 11\n",
            "Batch 18/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 14, filtered true_tags len: 24\n",
            "Batch 19/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 29, filtered true_tags len: 36\n",
            "Batch 20/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 43, filtered true_tags len: 17\n",
            "Batch 21/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 11, filtered true_tags len: 22\n",
            "Batch 22/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 17, filtered true_tags len: 28\n",
            "Batch 23/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 29, filtered true_tags len: 23\n",
            "Batch 24/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 31, filtered true_tags len: 21\n",
            "Batch 25/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 42, filtered true_tags len: 25\n",
            "Batch 26/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 22, filtered true_tags len: 15\n",
            "Batch 27/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 35, filtered true_tags len: 17\n",
            "Batch 28/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 31, filtered true_tags len: 11\n",
            "Batch 29/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 25, filtered true_tags len: 23\n",
            "Batch 30/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 17, filtered true_tags len: 37\n",
            "Batch 31/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 34, filtered true_tags len: 39\n",
            "Batch 32/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 19, filtered true_tags len: 23\n",
            "Batch 33/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 42, filtered true_tags len: 25\n",
            "Batch 34/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 26, filtered true_tags len: 25\n",
            "Batch 35/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 15, filtered true_tags len: 33\n",
            "Batch 36/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 24, filtered true_tags len: 9\n",
            "Batch 37/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 26, filtered true_tags len: 16\n",
            "Batch 38/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 17, filtered true_tags len: 7\n",
            "Batch 39/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 25, filtered true_tags len: 21\n",
            "Batch 40/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 41, filtered true_tags len: 29\n",
            "Batch 41/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 31, filtered true_tags len: 12\n",
            "Batch 42/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 18, filtered true_tags len: 38\n",
            "Batch 43/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 24, filtered true_tags len: 14\n",
            "Batch 44/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 18, filtered true_tags len: 10\n",
            "Batch 45/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 36, filtered true_tags len: 27\n",
            "Batch 46/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 21, filtered true_tags len: 29\n",
            "Batch 47/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 33, filtered true_tags len: 18\n",
            "Batch 48/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 32, filtered true_tags len: 27\n",
            "Batch 49/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 32, filtered true_tags len: 25\n",
            "Batch 50/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 42, filtered true_tags len: 30\n",
            "Batch 51/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 17, filtered true_tags len: 24\n",
            "Batch 52/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 23, filtered true_tags len: 30\n",
            "Batch 53/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 22, filtered true_tags len: 24\n",
            "Batch 54/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 24, filtered true_tags len: 14\n",
            "Batch 55/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 20, filtered true_tags len: 22\n",
            "Batch 56/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 12, filtered true_tags len: 20\n",
            "Batch 57/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 27, filtered true_tags len: 16\n",
            "Batch 58/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 24, filtered true_tags len: 25\n",
            "Batch 59/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 20, filtered true_tags len: 38\n",
            "Batch 60/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 28, filtered true_tags len: 20\n",
            "Batch 61/119\n",
            "⚠️ Length mismatch in sample 0\n",
            "  pred_tags len: 21, filtered true_tags len: 28\n",
            "Batch 62/119\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m attn_masks = batch[\u001b[33m'\u001b[39m\u001b[33mattn_masks\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m     10\u001b[39m labels = batch[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m].clone()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m _, pred_tag_seqs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(input_ids.size(\u001b[32m0\u001b[39m)):\n\u001b[32m     15\u001b[39m     pred_tags = pred_tag_seqs[i]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mBERT_Bi_CRF.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch.LongTensor, attention_mask: torch.LongTensor) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     features, masks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     scores, tag_seq = \u001b[38;5;28mself\u001b[39m.crf(features, masks)\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m scores, tag_seq\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mBERT_Bi_CRF.build_features\u001b[39m\u001b[34m(self, input_ids, attention_mask)\u001b[39m\n\u001b[32m     35\u001b[39m sorted_seq_length, perm_idx = seq_length.sort(descending=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     36\u001b[39m embeds = embeds[perm_idx]\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m pack_sequence = pack_padded_sequence(embeds, lengths=\u001b[43msorted_seq_length\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, batch_first=\u001b[38;5;28;01mTrue\u001b[39;00m, enforce_sorted=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     38\u001b[39m packed_output, _ = \u001b[38;5;28mself\u001b[39m.lstm(pack_sequence)\n\u001b[32m     39\u001b[39m lstm_out, _ = pad_packed_sequence(packed_output, batch_first=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(valid_dataloader):\n",
        "        print(f\"Batch {batch_idx}/{len(valid_dataloader)}\")\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attn_masks = batch['attn_masks'].to(device)\n",
        "        labels = batch['labels'].clone()\n",
        "\n",
        "        _, pred_tag_seqs = model(input_ids, attn_masks)\n",
        "\n",
        "        for i in range(input_ids.size(0)):\n",
        "            pred_tags = pred_tag_seqs[i]\n",
        "            true_tags = labels[i].tolist()\n",
        "\n",
        "            # Filter out special tokens ([CLS], [SEP]) labeled as -100\n",
        "            filtered_true_tags = []\n",
        "            for tag in true_tags:\n",
        "                if tag != -100:\n",
        "                    filtered_true_tags.append(tag)\n",
        "\n",
        "            # pred_tags naturally matches the filtered length\n",
        "            if len(pred_tags) != len(filtered_true_tags):\n",
        "                print(f\"⚠️ Length mismatch in sample {i}\")\n",
        "                print(f\"  pred_tags len: {len(pred_tags)}, filtered true_tags len: {len(filtered_true_tags)}\")\n",
        "                break\n",
        "\n",
        "            all_preds.extend(pred_tags)\n",
        "            all_labels.extend(filtered_true_tags)\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    print(f\" Accuracy: {acc:.4f} | 🏅 F1-score: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "⚠️ Skipping mismatched sample 1\n",
        "  pred_tags len: 34, true_tags len: 35\n",
        "⚠️ Skipping mismatched sample 2\n",
        "  pred_tags len: 16, true_tags len: 44\n",
        "⚠️ Skipping mismatched sample 3\n",
        "  pred_tags len: 9, true_tags len: 24\n",
        "⚠️ Skipping mismatched sample 4\n",
        "  pred_tags len: 32, true_tags len: 34\n",
        "⚠️ Skipping mismatched sample 7\n",
        "  pred_tags len: 24, true_tags len: 32\n",
        "⚠️ Skipping mismatched sample 9\n",
        "  pred_tags len: 17, true_tags len: 24\n",
        "⚠️ Skipping mismatched sample 10\n",
        "  pred_tags len: 24, true_tags len: 36\n",
        "⚠️ Skipping mismatched sample 12\n",
        "  pred_tags len: 35, true_tags len: 42\n",
        "⚠️ Skipping mismatched sample 13\n",
        "  pred_tags len: 17, true_tags len: 19\n",
        "⚠️ Skipping mismatched sample 19\n",
        "  pred_tags len: 24, true_tags len: 34\n",
        "⚠️ Skipping mismatched sample 21\n",
        "  pred_tags len: 20, true_tags len: 22\n",
        "⚠️ Skipping mismatched sample 25\n",
        "  pred_tags len: 24, true_tags len: 32\n",
        "⚠️ Skipping mismatched sample 26"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# length = 60(padded)\n",
        "labels[1] = tensor([-100,    0,    3,    7,    7,    7,    0,    0,    0,    0,    0,    0,\n",
        "           0,    0,    1,    5,    0,    0,    0,    0,    0,    2,    0,    0,\n",
        "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100, -100,\n",
        "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
        "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100])\n",
        "# length = 35\n",
        "attn_masks[1] = tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
        "# length = 34\n",
        "pred_tag_seqs[1] = [np.int64(0), np.int64(0), np.int64(3), np.int64(7), np.int64(3), np.int64(7), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(5), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), 0]\n",
        "# length = 35\n",
        "true_tags = [0, 0, 3, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-PXZ8TDjr-j"
      },
      "source": [
        "### Training CRF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW-XduXmPLns"
      },
      "source": [
        "### BERT-BiLSTM-CRF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPMiRf_iaX1O"
      },
      "source": [
        "From author: Cao Shuai ,\n",
        "Licnese : (C)Copyright 2018-2019, MILAB_SCU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utap0nHUfuhG"
      },
      "source": [
        "Utility functions used in the CRF layer to compute probabilities and decode sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpM18818PKeR"
      },
      "outputs": [],
      "source": [
        "# the vec's shape should be of 2-dimensions\n",
        "# Return tensor of indices, shape (batch_size,)\n",
        "def argmax(vec):\n",
        "  _,idx = torch.max(vec,1)\n",
        "  return idx.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cctef0fqTKqI"
      },
      "outputs": [],
      "source": [
        "# compute the highest score from LSTM-layer amongs the tagset, calculate the log-sum exp of the difference between\n",
        "# the score and the max_score\n",
        "\n",
        "def log_sum_exp(vec):\n",
        "  max_score = vec[0,argmax(vec)] # max_score = 5\n",
        "  max_score_broadcast = max_score.view(1,-1).expand(1,vec.size()[1])\n",
        "  return max_score + torch.log(torch.sum(torch.exp(vec-max_score_broadcast)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1MxYB4DNshz"
      },
      "source": [
        "#### The partition function is the sum of scores for all possible tag sequences, used to normalize probabilities in the CRF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfZGHiaNZSu4"
      },
      "outputs": [],
      "source": [
        "#A 3D tensor of shape (batch_size, seq_len, num_tags)\n",
        "# substract max for stability\n",
        "# this implements the log partition function\n",
        "def log_sum_exp_batch(log_tensor, axis=-1):\n",
        "    \"\"\"Compute log-sum-exp for a batch of tensors along the specified axis.\"\"\"\n",
        "    max_score = torch.max(log_tensor, axis)[0]\n",
        "    max_score_expanded = max_score.view(log_tensor.shape[0], -1, 1)\n",
        "    return max_score + torch.log(torch.exp(log_tensor - max_score_expanded).sum(axis))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hro_6GYhcukB",
        "outputId": "9c4cc21e-e4ed-4940-e1ab-8ec6706d52b9"
      },
      "outputs": [],
      "source": [
        "# gather : select elements from a tensor using dynamic indices\n",
        "x = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6]])\n",
        "\n",
        "index = torch.tensor([[1,0,2], [2,0,1]])  # Shape: (2, 1)\n",
        "\n",
        "out = torch.gather(x, dim=1, index=index)  # dim=1 → columns\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Al2mNXaMgCmH"
      },
      "outputs": [],
      "source": [
        "class Bert_BiLSTM_CRF(nn.Module):\n",
        "    def __init__(self, bert: BertModel, tag_to_idx: dict, hidden_dim: int = 768):\n",
        "        \"\"\"\n",
        "        Initialize BERT + Bi-LSTM + CRF model for NER.\n",
        "\n",
        "        Args:\n",
        "            bert (BertModel): Pretrained BERT model.\n",
        "            tag_to_idx (dict): Mapping of tags to indices (e.g., {'O': 0, 'B-PER': 1, ...}).\n",
        "            hidden_dim (int): Hidden dimension of Bi-LSTM (default: 768).\n",
        "        \"\"\"\n",
        "        super(Bert_BiLSTM_CRF, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.tag_to_idx = tag_to_idx\n",
        "        self.tagset_size = len(tag_to_idx)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Bi-LSTM: 2 layers, bidirectional, input size = BERT hidden size (768)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=768,\n",
        "            hidden_size=hidden_dim // 2,\n",
        "            num_layers=2,\n",
        "            bidirectional=True,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Linear layer to map LSTM output to tag space\n",
        "        self.fc = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # CRF transition matrix\n",
        "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # Ensure [CLS] and [SEP] are in tag_to_idx\n",
        "        if '[CLS]' not in tag_to_idx or '[SEP]' not in tag_to_idx:\n",
        "            raise ValueError(\"tag_to_idx must include '[CLS]' and '[SEP]'\")\n",
        "        self.start_label_id = tag_to_idx['[CLS]']\n",
        "        self.end_label_id = tag_to_idx['[SEP]']\n",
        "\n",
        "        # Initialize transition scores: disallow invalid transitions\n",
        "        self.transitions.data[self.start_label_id, :] = -10000.0\n",
        "        self.transitions.data[:, self.end_label_id] = -10000.0\n",
        "        self.to(self.device)\n",
        "\n",
        "    def init_hidden(self, batch_size: int):\n",
        "        \"\"\"Initialize LSTM hidden state.\"\"\"\n",
        "        return (\n",
        "            torch.randn(4, batch_size, self.hidden_dim // 2).to(self.device),  # 4 = 2 layers * 2 directions\n",
        "            torch.randn(4, batch_size, self.hidden_dim // 2).to(self.device)\n",
        "        )\n",
        "\n",
        "    def _bert_enc(self, input_ids: torch.LongTensor, attention_mask: torch.LongTensor):\n",
        "        \"\"\"\n",
        "        Encode inputs using BERT.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.LongTensor): Shape (batch_size, seq_len)\n",
        "            attention_mask (torch.LongTensor): Shape (batch_size, seq_len)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Shape (batch_size, seq_len, 768)\n",
        "        \"\"\"\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        return outputs.last_hidden_state\n",
        "\n",
        "    def _get_lstm_features(self, input_ids: torch.LongTensor, attention_mask: torch.LongTensor):\n",
        "        \"\"\"Generate emission scores from BERT and Bi-LSTM.\"\"\"\n",
        "        embeds = self._bert_enc(input_ids, attention_mask)  # (batch_size, seq_len, 768)\n",
        "        batch_size = input_ids.size(0)\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "        lstm_out, _ = self.lstm(embeds, hidden)  # (batch_size, seq_len, hidden_dim)\n",
        "        lstm_feats = self.fc(lstm_out)  # (batch_size, seq_len, tagset_size)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _forward_alg(self, feats: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Forward algorithm to compute log partition function (log Z(x)).\n",
        "\n",
        "        Args:\n",
        "            feats (torch.Tensor): Emission scores, shape (batch_size, seq_len, tagset_size)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Log partition function, shape (batch_size, 1)\n",
        "        \"\"\"\n",
        "        T = feats.shape[1]\n",
        "        batch_size = feats.shape[0]\n",
        "\n",
        "        log_alpha = torch.full((batch_size, 1, self.tagset_size), -10000.0, device=self.device)\n",
        "        log_alpha[:, 0, self.start_label_id] = 0.0\n",
        "\n",
        "        for t in range(1, T):\n",
        "            log_alpha = (log_sum_exp_batch(self.transitions + log_alpha, axis=-1) + feats[:, t]).unsqueeze(1)\n",
        "\n",
        "        return log_sum_exp_batch(log_alpha)\n",
        "\n",
        "    def _score_sentence(self, feats: torch.Tensor, label_ids: torch.LongTensor):\n",
        "        \"\"\"\n",
        "        Compute score of given tag sequence.\n",
        "\n",
        "        Args:\n",
        "            feats (torch.Tensor): Emission scores, shape (batch_size, seq_len, tagset_size)\n",
        "            label_ids (torch.LongTensor): True tag indices, shape (batch_size, seq_len)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Score of tag sequence, shape (batch_size, 1)\n",
        "        \"\"\"\n",
        "        T = feats.shape[1]\n",
        "        batch_size = feats.shape[0]\n",
        "        score = torch.zeros((batch_size, 1), device=self.device)\n",
        "\n",
        "        for t in range(1, T):\n",
        "            mask = (label_ids[:, t] != -100) & (label_ids[:, t-1] != -100)\n",
        "            score[mask] = score[mask] + \\\n",
        "                self.transitions[label_ids[mask, t-1], label_ids[mask, t]].view(-1, 1) + \\\n",
        "                feats[mask, t].gather(-1, label_ids[mask, t].view(-1, 1))\n",
        "\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Viterbi algorithm to find the most likely tag sequence.\n",
        "\n",
        "        Args:\n",
        "            feats (torch.Tensor): Emission scores, shape (batch_size, seq_len, tagset_size)\n",
        "\n",
        "        Returns:\n",
        "            tuple: (max_logLL, path)\n",
        "                - max_logLL: Log probability of best sequence, shape (batch_size,)\n",
        "                - path: Best tag sequence, shape (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "        T = feats.shape[1]\n",
        "        batch_size = feats.shape[0]\n",
        "\n",
        "        log_delta = torch.full((batch_size, 1, self.tagset_size), -10000.0, device=self.device)\n",
        "        log_delta[:, 0, self.start_label_id] = 0.0\n",
        "        psi = torch.zeros((batch_size, T, self.tagset_size), dtype=torch.long, device=self.device)\n",
        "\n",
        "        for t in range(1, T):\n",
        "            log_delta, psi[:, t] = torch.max(self.transitions + log_delta, -1)\n",
        "            log_delta = (log_delta + feats[:, t]).unsqueeze(1)\n",
        "\n",
        "        path = torch.zeros((batch_size, T), dtype=torch.long, device=self.device)\n",
        "        max_logLL, path[:, -1] = torch.max(log_delta.squeeze(), -1)\n",
        "\n",
        "        for t in range(T-2, -1, -1):\n",
        "            path[:, t] = psi[:, t+1].gather(-1, path[:, t+1].view(-1, 1)).squeeze()\n",
        "\n",
        "        return max_logLL, path\n",
        "\n",
        "    def neg_log_likelihood(self, input_ids: torch.LongTensor, attention_mask: torch.LongTensor, labels: torch.LongTensor):\n",
        "        \"\"\"\n",
        "        Compute negative log-likelihood loss.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.LongTensor): Shape (batch_size, seq_len)\n",
        "            attention_mask (torch.LongTensor): Shape (batch_size, seq_len)\n",
        "            labels (torch.LongTensor): Shape (batch_size, seq_len)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Mean loss across batch\n",
        "        \"\"\"\n",
        "        feats = self._get_lstm_features(input_ids, attention_mask)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, labels)\n",
        "        return torch.mean(forward_score - gold_score)\n",
        "\n",
        "    def forward(self, input_ids: torch.LongTensor, attention_mask: torch.LongTensor, labels: torch.LongTensor = None):\n",
        "        \"\"\"\n",
        "        Forward pass for training or inference.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.LongTensor): Shape (batch_size, seq_len)\n",
        "            attention_mask (torch.LongTensor): Shape (batch_size, seq_len)\n",
        "            labels (torch.LongTensor, optional): Shape (batch_size, seq_len)\n",
        "\n",
        "        Returns:\n",
        "            dict: {'loss': torch.Tensor} for training, {'score': torch.Tensor, 'tag_seq': torch.Tensor} for inference\n",
        "        \"\"\"\n",
        "        feats = self._get_lstm_features(input_ids, attention_mask)\n",
        "        if labels is not None:\n",
        "            loss = self.neg_log_likelihood(input_ids, attention_mask, labels)\n",
        "            return {\"loss\": loss, \"logits\": feats}\n",
        "        score, tag_seq = self._viterbi_decode(feats)\n",
        "        return {\"score\": score, \"tag_seq\": tag_seq}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_chMdDs69zW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GB1ecR0lMNUb",
        "outputId": "cde0dbb8-873f-487d-a2ec-4e9fc21644f6"
      },
      "outputs": [],
      "source": [
        "label2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5cnC7wyMTS8",
        "outputId": "3f1ac392-b9fc-4258-d4f7-72a54046a374"
      },
      "outputs": [],
      "source": [
        "id2label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF9uiaktMAcv"
      },
      "outputs": [],
      "source": [
        "mo = BiLSTM_CRF_Tagger_BERT(bert_model,label2id,hidden_dim= 128, num_layers = 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DfRNyC375ZZ"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6ik7lJI9OL_",
        "outputId": "98215748-e490-42f9-b617-8d100fb6af75"
      },
      "outputs": [],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36cCvagIWZab"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDW0PEsBgNWc"
      },
      "outputs": [],
      "source": [
        "# Example usage with your DataLoader\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 16\n",
        "TEST_BATCH_SIZE = 16\n",
        "\n",
        "train_params = {\n",
        "    'batch_size': TRAIN_BATCH_SIZE,\n",
        "    'shuffle': True\n",
        "}\n",
        "valid_params = {\n",
        "    'batch_size': VALID_BATCH_SIZE,\n",
        "    'shuffle': False\n",
        "}\n",
        "test_params = {\n",
        "    'batch_size': TEST_BATCH_SIZE,\n",
        "    'shuffle': False\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JGvaT1O8g_up",
        "outputId": "9942065c-c72f-4377-db1e-f89ff955315c"
      },
      "outputs": [],
      "source": [
        "train_dataset[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gLfm0FhkMIt"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel\n",
        "bert = build_bert_model()\n",
        "\n",
        "model = Bert_BiLSTM_CRF(bert, label2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7Tv_4Ncs-pw",
        "outputId": "2bf1bee7-c786-48f6-bce0-10e0d433d3f9"
      },
      "outputs": [],
      "source": [
        "bert.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F8y4JyakG-x"
      },
      "outputs": [],
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "num_training_steps = len(train_dataloader) * 3  # 3 epochs\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(0.1 * num_training_steps),\n",
        "    num_training_steps=num_training_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vou144dIpuwH",
        "outputId": "b7b46704-91ed-4a41-9875-fb62e5289ea5"
      },
      "outputs": [],
      "source": [
        "train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_tJIRTXppQT",
        "outputId": "40c65754-78bb-4caa-cc1d-cc85cb8ac093"
      },
      "outputs": [],
      "source": [
        "len(train_dataloader)*16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rl8_D7krkOQV",
        "outputId": "73f04a4b-e102-4097-efa6-19dd1601153e"
      },
      "outputs": [],
      "source": [
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for index, batch in enumerate(train_dataloader):\n",
        "        if index % 40 == 0:\n",
        "          print(f\"Batch{index+1} out of {len(train_dataloader)}\")\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attn_masks = batch['attn_masks'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attn_masks, labels=labels)\n",
        "        loss = outputs['loss']\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "    loss_avg = total_loss/len(train_dataloader)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss_avg:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zI9rBOSe6on"
      },
      "source": [
        "### Vertibi\n",
        "#### Purpose\n",
        "This function implements the Viterbi algorithm to find the most likely tag sequence ($ \\arg\\max P(y|x) $) for a batch of sequences, given emission scores (feats)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgMeFgRUGquK"
      },
      "source": [
        "### EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrML9KUVGoFv"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self,epoch_limit = 5, improv_threshold = 0):\n",
        "        self.epoch_limit = epoch_limit\n",
        "        self.improv_threshold = improv_threshold  #minimum change\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.counter = 0\n",
        "        self.best_model_state = None\n",
        "    # create our own callable to facilitate configuration\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = val_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.best_model_state = model.state_dict()  #keep track of the model state (parameters(weights and biases))\n",
        "        elif score > self.best_score + self.improv_threshold:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.epoch_limit:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.best_model_state = model.state_dict()\n",
        "            self.counter = 0  # reset counter to zero\n",
        "\n",
        "    def load_best_model(self, model):\n",
        "        model.load_state_dict(self.best_model_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wC2qmsXRc6WW"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_loop_bilstm(model, train_dataloader, optimizer, scheduler, epochs,\n",
        "                      device=torch.device('cpu'), seed_val=42, tagset_size=None):\n",
        "    random.seed(seed_val)\n",
        "    np.random.seed(seed_val)\n",
        "    torch.manual_seed(seed_val)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "    # Move model to device\n",
        "    model.to(device)\n",
        "    best_model_state = copy.deepcopy(model.state_dict())\n",
        "    min_train_loss = float('inf')\n",
        "    loss_values_train = []\n",
        "    loss_values_valid = []\n",
        "\n",
        "    # Define loss function (already included in model, but kept here for clarity)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    for epoch_i in range(epochs):\n",
        "        print(f'======== Epoch {epoch_i + 1} / {epochs} ========')\n",
        "        print('Training...')\n",
        "        total_loss = 0.0\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            if step % 40 == 0 and step != 0:\n",
        "                print(f'  Batch {step} of {len(train_dataloader)}')\n",
        "\n",
        "            # Get inputs from NerDataset_BERT\n",
        "            input_ids = batch['input_ids'].to(device)  # Shape: (B, S)\n",
        "            attn_masks = batch['attn_masks'].to(device)  # Shape: (B, S)\n",
        "            labels = batch['labels'].to(device)  # Shape: (B, S)\n",
        "\n",
        "            # Compute actual lengths from attention_mask\n",
        "            lengths = attn_masks.sum(dim=-1).to('cpu')  # Number of non-padded tokens per sequence\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            #forward pass (using revised BiLSTMTagger_BERT)\n",
        "            outputs = model(input_ids, attention_mask=attn_masks, lengths=lengths, labels=labels)\n",
        "            loss = outputs['loss']  # Loss is computed in the model's forward method\n",
        "\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip gradients to prevent exploding gradients\n",
        "            # this keeps the direction of gradient while clipping the gradient to be between (-1,1)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # we may use batch normalization, but it is more complicated in RNN\n",
        "            optimizer.step() # update parameter\n",
        "            scheduler.step() # update learning rate to have suitable early training and late training\n",
        "\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "        loss_values_train.append(avg_train_loss)\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        with torch.inference_mode():\n",
        "          for step, batch in enumerate(valid_dataloader):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attn_masks = batch[\"attn_masks\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            lengths = attn_masks.sum(dim = -1).to('cpu')\n",
        "            predict = model(input_ids, attn_masks,lengths, labels)\n",
        "            valid_loss = outputs[\"loss\"]\n",
        "            total_valid_loss += valid_loss.item()\n",
        "          avg_valid_loss = total_valid_loss/len(valid_dataloader)\n",
        "        loss_values_valid.append(avg_valid_loss)\n",
        "        early_stopping(valid_loss_avg, model)\n",
        "        if early_stopping.early_stop:\n",
        "          print(\"Early stopping\")\n",
        "          best_model_state = early_stopping.best_model_state(model)\n",
        "          break\n",
        "\n",
        "        print(f\"  Average training loss: {avg_train_loss:.4f}, Average validation loss: {avg_valid_loss: .4f}\")\n",
        "\n",
        "    # Load best model state\n",
        "    model.load_state_dict(best_model_state)\n",
        "    return loss_values_valid,  model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCZYQbZpio0z"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "#initialize model and dataloader\n",
        "model = BiLSTMTagger_BERT(bert_model,tagset_size=TAGSET_SIZE)\n",
        "#optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eO7WuVK20n59"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "ij_BzzumdMis",
        "outputId": "6d8bc5e7-6c9d-4be8-fb5e-0ea21c333132"
      },
      "outputs": [],
      "source": [
        "\n",
        "#train\n",
        "loss_valid_values,best_model = train_loop_bilstm(\n",
        "    model=model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    epochs=2,\n",
        "    device=device,\n",
        "    seed_val=42,\n",
        "    tagset_size=TAGSET_SIZE\n",
        ")\n",
        "\n",
        "print(\"Training complete. Loss values:\", loss_valid_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGxxNUn9y7y5"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict, \"BiLSTM_PT_Soft_BERT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cy4Ndm8p-tf"
      },
      "source": [
        "### Optimizer - AdamW:\n",
        "\n",
        "AdamW is a stochastic optimization method that modifies the typical implementation of weight decay in Adam, by decoupling weight decay from the gradient update.\n",
        "\n",
        "Learning Rate:\n",
        "\n",
        "Learning rate is a tuning parameter in machine learning and statistics that controls how much a model's parameters adjust during each iteration of an optimization algorithm. It's a floating point number that's usually between 0.01 and 0.1\n",
        "\n",
        "Adam Epsilon:\n",
        "\n",
        "The parameter epsilon shows up in the update step.\n",
        "\n",
        "θ_t <- θ_{t-1} - α • mhat_t / (sqrt(vhat_t) + ε)\n",
        "\n",
        "It is primarily used as a guard against a zero second second moment causing a division by zero case. If it is too large it will bias the moment estimation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77LLl925t7id"
      },
      "outputs": [],
      "source": [
        "def eval_fn(model, test_dataloader):\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(test_dataloader):\n",
        "\n",
        "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['attn_masks'].to(device, dtype = torch.long)\n",
        "            targets = batch['labels'].to(device, dtype = torch.long)\n",
        "\n",
        "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "            loss, eval_logits = outputs.loss, outputs.logits\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += targets.size(0)\n",
        "\n",
        "            if idx % 40==0:\n",
        "                loss_step = eval_loss/nb_eval_steps\n",
        "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
        "\n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "\n",
        "            # now, use mask to determine where we should compare predictions with\n",
        "            # targets (includes [CLS] and [SEP] token predictions)\n",
        "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
        "\n",
        "            # actual targets\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy).cpu().numpy()\n",
        "            # actual preicted labels\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy).cpu().numpy()\n",
        "\n",
        "\n",
        "            real_tar_ = []\n",
        "            real_pred_ = []\n",
        "\n",
        "            for t,p in zip(targets, predictions):\n",
        "                if t != -100:\n",
        "                    real_tar_.append(t)\n",
        "                    real_pred_.append(p)\n",
        "\n",
        "            eval_labels.extend(real_tar_)\n",
        "            eval_preds.extend(real_pred_)\n",
        "\n",
        "            tmp_eval_accuracy = accuracy_score(real_tar_, real_pred_)\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return eval_labels, eval_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DS0-l32Su-ER",
        "outputId": "c48838ca-7311-4bc8-8215-02c5b7182049"
      },
      "outputs": [],
      "source": [
        "true_labels, pred_labels = eval_fn(best_model, valid_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u06X-YHivKkQ"
      },
      "outputs": [],
      "source": [
        "def print_full_classification_report(true_labels, pred_labels, zero_division=np.nan):\n",
        "    # computing the accuracy\n",
        "    accuracy = accuracy_score(y_true=true_labels, y_pred=pred_labels, )\n",
        "\n",
        "    print(\"\\naccuracy score {:.3%}\".format(accuracy))\n",
        "\n",
        "    # compute the macro precision\n",
        "    macro_precision = precision_score(y_true=true_labels, y_pred=pred_labels, average='macro',\n",
        "                                      zero_division=zero_division)\n",
        "    # compute the macro recall\n",
        "    macro_recall = recall_score(y_true=true_labels, y_pred=pred_labels, average='macro',)\n",
        "    # compute the macro f1_score\n",
        "    macro_f1_score = f1_score(y_true = true_labels, y_pred = pred_labels, average='macro')\n",
        "\n",
        "    print(\"\\n----- macro scores --------\")\n",
        "    print(\"macro precision score: {:.3%}\".format(macro_precision))\n",
        "    print(\"macro recall score: {:.3%}\".format(macro_recall))\n",
        "    print(\"macro f1 score: {:.3%}\".format(macro_f1_score))\n",
        "\n",
        "    # ---- micro scores -------\n",
        "    micro_precision = precision_score(y_true=true_labels, y_pred=pred_labels, average='micro',\n",
        "                                      zero_division=zero_division)\n",
        "\n",
        "    micro_recall = recall_score(y_true=true_labels, y_pred=pred_labels, average='micro')\n",
        "\n",
        "    micro_f1_score = f1_score(y_true = true_labels, y_pred = pred_labels, average='micro')\n",
        "\n",
        "\n",
        "    print(\"\\n----- micro scores --------\")\n",
        "    print(\"micro precision score: {:.2%}\".format(micro_precision))\n",
        "    print(\"micro recall score: {:.2%}\".format(micro_recall))\n",
        "    print(\"micro f1 score: {:.2%}\".format(micro_f1_score))\n",
        "\n",
        "    # ------ weighted scores------\n",
        "    wt_precision = precision_score(y_true=true_labels, y_pred=pred_labels, average='weighted',\n",
        "                                             zero_division=zero_division)\n",
        "\n",
        "    wt_recall = recall_score(y_true=true_labels, y_pred=pred_labels, average='weighted')\n",
        "\n",
        "    wt_f1_score = f1_score(y_true = true_labels, y_pred = pred_labels, average='weighted')\n",
        "\n",
        "    print(\"\\n----- weighted scores --------\")\n",
        "    print(\"weighted precision score: {:.2%}\".format(wt_precision))\n",
        "    print(\"weighted recall score: {:.2%}\".format(wt_recall))\n",
        "    print(\"weighted f1 score: {:.2%}\".format(wt_f1_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7xZaiKohvPV4",
        "outputId": "67c66c46-2ab3-4f10-df77-804bfce0ed6e"
      },
      "outputs": [],
      "source": [
        "print(\"\\nEvaluation Metrics for Validation DataSet\")\n",
        "print(\"=========================================\")\n",
        "print_full_classification_report(true_labels, pred_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tuRIooEBvTSn",
        "outputId": "e549325d-0ed0-4f67-af55-0ffb97a8d4d2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "clf_report_for_valid = classification_report(y_true=true_labels,\n",
        "                                             y_pred=pred_labels,\n",
        "                                             labels=[x for x in range(len(label2id))],\n",
        "                                             target_names=list(label2id.keys()),\n",
        "                                             zero_division=0\n",
        "                                            )\n",
        "print(\"\\nLabel-Wise Evaluation Metrics for Validation DataSet\")\n",
        "print(\"Classification Report for each label\\n\")\n",
        "print(clf_report_for_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "collapsed": true,
        "id": "u4H_XfEyvbs2",
        "outputId": "152e3693-df71-485e-b19a-a482b1093ea2"
      },
      "outputs": [],
      "source": [
        "label_names_ = list(label2id.keys())\n",
        "label_ids_ = [i for i in range(len(label_names_))]\n",
        "\n",
        "# compute confusion matrix collectively\n",
        "cm_for_valid = confusion_matrix(y_true=true_labels,\n",
        "                                y_pred=pred_labels,\n",
        "                                labels=label_ids_,\n",
        "                               )\n",
        "\n",
        "# compute NORMALIZE (row) confusion matrix collectively\n",
        "cm_for_valid_norm = confusion_matrix(y_true=true_labels,\n",
        "                                y_pred=pred_labels,\n",
        "                                labels=label_ids_,\n",
        "                                normalize='true'\n",
        "                               )\n",
        "\n",
        "df_cm_for_valid = pd.DataFrame(cm_for_valid, index=label_names_, columns=label_names_)\n",
        "df_cm_for_valid_norm = pd.DataFrame(cm_for_valid_norm, index=label_names_, columns=label_names_)\n",
        "\n",
        "print(\"Confusion Matrix DataFrame\")\n",
        "display(df_cm_for_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "GjYv_2Acvkts",
        "outputId": "c8770161-95a1-4e40-86b3-f9ef43ee621f"
      },
      "outputs": [],
      "source": [
        "# compute multilabel confusion matrix\n",
        "# Here it will compute cm at the class level\n",
        "# labels are binarized under a one-vs-rest way\n",
        "mcm_for_valid = multilabel_confusion_matrix(y_true=true_labels,\n",
        "                                        y_pred=pred_labels,\n",
        "                                        labels=label_ids_,\n",
        "                                       )\n",
        "\n",
        "# Label wise confusin matrix (one vs all fashion)\n",
        "for label_, l_cm in zip(label_names_, mcm_for_valid):\n",
        "    col_names_ = [f\"not_{label_}\",label_]\n",
        "    print(f\"Validation Dataset - Confusion Matrix for {label_}\")\n",
        "    ldf = pd.DataFrame(l_cm, index=col_names_, columns=col_names_)\n",
        "    display(ldf)\n",
        "    print(\"-----------------------------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaWss7jovzUC"
      },
      "outputs": [],
      "source": [
        "def create_input_ids(sentence, tokenizer):\n",
        "    \"\"\"\n",
        "    Create sample input ids for a given sentence using the tokenizer\n",
        "    \"\"\"\n",
        "    # Encoding and convert the sentences into tensors\n",
        "    sample_sentence = tokenizer.encode(' '.join(sentence))\n",
        "    sample_input_ids = torch.tensor([sample_sentence])\n",
        "\n",
        "    return sample_input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rOe0nahv3jF"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "def make_raw_prediction(sentence, model, tokenizer, unseen_word_label='O', device='cpu'):\n",
        "    # Function which retrieves key value for our Label Dictionary\n",
        "    def get_key(val):\n",
        "        for key, value in label2id.items():\n",
        "             if val == value:\n",
        "                 return key\n",
        "        return unseen_word_label\n",
        "\n",
        "    try:\n",
        "        device = torch.device(device)\n",
        "    except:\n",
        "        print(f\"unable to move to device {device}. Hence using cpu.\")\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "    # move model to device\n",
        "    model.to(device)\n",
        "    # Create sample input ids\n",
        "    sample_input_ids = create_input_ids(sentence, tokenizer)\n",
        "    # move the input ids to device\n",
        "    sample_input_ids.to(device)\n",
        "    # Predicting the test sample using model() function\n",
        "    with torch.no_grad():\n",
        "        output = model(sample_input_ids)\n",
        "\n",
        "    # Extract the labels\n",
        "    label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
        "    #Tokenize\n",
        "    tokens = tokenizer.convert_ids_to_tokens(sample_input_ids.to('cpu').numpy()[0])\n",
        "\n",
        "    # We need to club wordpieces starting with \"##' in a single word\n",
        "    new_tokens, new_label = [], []\n",
        "    for token, label_idx in zip(tokens, label_indices[0]):\n",
        "        if token.startswith(\"##\"):\n",
        "            new_tokens[-1] = new_tokens[-1] + token[2:]\n",
        "        else:\n",
        "            new_label.append(get_key(label_idx))\n",
        "            new_tokens.append(token)\n",
        "\n",
        "    #Appending Tokens and Labels\n",
        "    sample_token, sample_label = [], []\n",
        "    for token, label in zip(new_tokens, new_label):\n",
        "        sample_token.append(token)\n",
        "        sample_label.append(label)\n",
        "\n",
        "    return sample_token, sample_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoyQ2qrnv77R"
      },
      "outputs": [],
      "source": [
        "def preprocess_raw_prediction(test_sentence, sample_token, sample_label):\n",
        "    # remove special tokens and corresponding labels\n",
        "    # they are not required\n",
        "    filter_tokens = sample_token[1:-1]\n",
        "    filter_labels = sample_label[1:-1]\n",
        "\n",
        "    # Result stored in list as token, label pair\n",
        "    res = []\n",
        "    # initial index for list of sample tokens\n",
        "    tok_idx = 0\n",
        "\n",
        "    # iterate through each token of test sentence\n",
        "    for word in test_sentence:\n",
        "        # pair to store the current word and its label\n",
        "        pair = tuple()\n",
        "        # check the length of current word\n",
        "        word_len = len(word)\n",
        "        # check the length of current token in sample tokens\n",
        "        tok_len = len(filter_tokens[tok_idx])\n",
        "\n",
        "        # Note: We are using the strategy that will\n",
        "        # assume that 1st word piece label is the\n",
        "        # label for the entire word.\n",
        "        # Hence, get the label for current token\n",
        "        tok_labl =  filter_labels[tok_idx]\n",
        "\n",
        "        # update the pair\n",
        "        pair = (word, tok_labl)\n",
        "\n",
        "        # increment the idx till word_len becomes equal to tok len\n",
        "        while word_len > tok_len:\n",
        "            # increment token idex\n",
        "            tok_idx +=1\n",
        "            # add the length of next token\n",
        "            tok_len += len(filter_tokens[tok_idx])\n",
        "        tok_idx+=1\n",
        "        res.append(pair)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwrH3QdHwAMw"
      },
      "outputs": [],
      "source": [
        "def make_prediction(sentence, model, tokenizer, unseen_word_label='O', sent_delimiter=\" \", device='cpu'):\n",
        "\n",
        "    # check if sentence is a list of words\n",
        "    if isinstance(sentence, str):\n",
        "        # create the string\n",
        "        sentence = sentence.strip().split(sent_delimiter)\n",
        "\n",
        "    # compute raw predictions\n",
        "    sample_token, sample_label = make_raw_prediction(sentence, model, tokenizer, unseen_word_label, device)\n",
        "\n",
        "    # preprocess raw prediction to make clean predictions\n",
        "    clean_pred_ = preprocess_raw_prediction(sentence, sample_token, sample_label)\n",
        "\n",
        "    return clean_pred_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFyLVBaLwGEg",
        "outputId": "2b9ef933-fc68-4945-93a3-43b38e41f131"
      },
      "outputs": [],
      "source": [
        "sent_ = input(\"Enter any sentence:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgiFp20vwKYH",
        "outputId": "c333b718-d0c4-4668-fbc7-1b5c1bb85b09"
      },
      "outputs": [],
      "source": [
        "print(f\"The sentence entered is:\\n {sent_}\")\n",
        "\n",
        "# compute label predictions\n",
        "pred_labels_ = make_prediction(sent_, model=best_model, tokenizer=tokenizer, device='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "collapsed": true,
        "id": "xeAmVE9lwPBM",
        "outputId": "e11f73e8-f614-40d9-9e60-d29bc0d6a784"
      },
      "outputs": [],
      "source": [
        "pred_df_ = pd.DataFrame(data=pred_labels_, columns=['Token', 'Labels'])\n",
        "pred_df_.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BP-1UtxJK0yP"
      },
      "outputs": [],
      "source": [
        "# Tokenize sentences (whitespace)\n",
        "def tokenize(sentence):\n",
        "    return sentence.strip().split()\n",
        "# Split labels\n",
        "def split_labels(label_str):\n",
        "    return label_str.strip().split(',')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeyAna7AK0yP"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_df['tokens'] = train_df['sentence'].apply(tokenize)\n",
        "train_df['label_list'] = train_df['labels'].apply(split_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "collapsed": true,
        "id": "QWwg30olK0yP",
        "outputId": "c5301f63-0810-4acb-f1b7-0a8b73669ad3"
      },
      "outputs": [],
      "source": [
        "train_df[train_df[\"tokens\"].apply(len) >50]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6m4pauoK0yP"
      },
      "source": [
        "### ID tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMFKjiXVK0yP"
      },
      "source": [
        "### We can use Text vectorization\n",
        "\n",
        "# Text vectorization consiste of parts:\n",
        "<li>\n",
        "    Standardization\n",
        "</li>\n",
        "<li>\n",
        "    Tokenization\n",
        "</li>\n",
        "<li>\n",
        "    Vocabulary building\n",
        "</li>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoGKtnQLK0yP"
      },
      "source": [
        "### Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm4o8YIWK0yP"
      },
      "source": [
        "### Necessary dictionaries for encoding and decoding phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvkAUFcyK0yP"
      },
      "outputs": [],
      "source": [
        "\n",
        "label2id = {k:v+1 for v,k in enumerate(all_labels)}\n",
        "id2label = {k+1:v for k,v in enumerate(all_labels)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbRH_lK0K0yP"
      },
      "outputs": [],
      "source": [
        "label2id[\"<PAD>\"] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7iVfkPLK0yP",
        "outputId": "ab44f0e9-e157-4d05-a3fe-5dbe4001e861"
      },
      "outputs": [],
      "source": [
        "label2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pojKPzHsK0yQ"
      },
      "outputs": [],
      "source": [
        "word_counts = Counter(word for tokens in train_df['tokens'] for word in tokens)\n",
        "word2id = {word: i + 2 for i, word in enumerate(word_counts)} # turn words into unique ids (+2) because give 0,1 to UNK and PAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UySpmf_aK0yQ"
      },
      "outputs": [],
      "source": [
        "id2word = {i+2:word for i,word in enumerate(word_counts)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0GEuz0iK0yQ"
      },
      "source": [
        "### Add '<PAD>' to use when padding and '<UNK>' in the case where there are unknown entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ACG885JK0yQ"
      },
      "outputs": [],
      "source": [
        "word2id['<PAD>'] = 0\n",
        "word2id['<UNK>'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlKmvO56K0yQ",
        "outputId": "577099bf-3c72-4fde-cde4-cfdd53f57c1e"
      },
      "outputs": [],
      "source": [
        "fallback = '<UNK>'\n",
        "fallback_id = word2id.get('B-gpe', list(label2id.values())[0])  ## get idea of a word\n",
        "\n",
        "fallback_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rz4gvSzDK0yQ",
        "outputId": "a76d5043-3cbe-4ab2-bfb4-933a4339bb01"
      },
      "outputs": [],
      "source": [
        "pad_token = '<PAD>'\n",
        "pad_id = label2id.get(pad_token, list(label2id.values())[0])\n",
        "pad_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhQhoJ5OK0yQ"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 40\n",
        "\n",
        "def encode(tokens, vocab, pad_len=40, fallback='<UNK>', pad_token='<PAD>'):\n",
        "    \"\"\"\n",
        "    Encodes a list of tokens using a vocabulary.\n",
        "\n",
        "    - tokens: List[str] → tokens or labels (e.g., words or NER tags)\n",
        "    - vocab: dict → maps token to index\n",
        "    - pad_len: int → fixed output length (pad or truncate)\n",
        "    - fallback: str → token to use if one isn't found (only relevant for input)\n",
        "    - pad_token: str → token to use for padding\n",
        "\n",
        "    Returns: List[int] of length `pad_len`\n",
        "    \"\"\"\n",
        "    fallback_id = vocab.get(fallback, 0)     # Use <UNK> if token not found\n",
        "    pad_id = vocab.get(pad_token, 0)         # Use <PAD> for padding\n",
        "\n",
        "    # Map each token to its ID\n",
        "    ids = [vocab.get(tok, fallback_id) for tok in tokens]\n",
        "\n",
        "    # Truncate or pad to fixed length\n",
        "    if len(ids) > pad_len:\n",
        "        return ids[:pad_len]\n",
        "    else:\n",
        "        return ids + [pad_id] * (pad_len - len(ids))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCD9d3ZpK0yQ"
      },
      "outputs": [],
      "source": [
        "X = np.array([encode(tokens, word2id, fallback='<UNK>', pad_token='<PAD>') for tokens in train_df['tokens']])\n",
        "y = np.array([encode(labels, label2id, fallback='O', pad_token= '<PAD>') for labels in train_df['label_list']])\n",
        "vocab_size = len(word2id)\n",
        "label_size = len(label2id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qpav1VYBK0yQ",
        "outputId": "ef0d1d3e-6408-4d0e-ca3e-943a9af8f304"
      },
      "outputs": [],
      "source": [
        "X[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usqPS-ayK0yQ",
        "outputId": "c2eb3282-42a1-4ede-b161-96d1a916c087"
      },
      "outputs": [],
      "source": [
        "## Example:\n",
        "sentence = train_df['tokens'][2]\n",
        "label    = train_df['label_list'][2]\n",
        "encoded_sentence = encode(sentence,word2id,fallback= '<UNK>', pad_token = '<PAD>')\n",
        "encoded_label  = encode(label, label2id, fallback = 'O', pad_token = '<PAD>')\n",
        "print(f\"Sentence : {sentence} with length {len(sentence)}\\n\",\n",
        "       f\"encoded sentence : {encoded_sentence} with length {len(encoded_sentence)}\\n\")\n",
        "\n",
        "print(f\"Label : {label} with length {len(label)}\\n\",\n",
        "       f\"encoded label : {encoded_label} with length {len(encoded_label)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTXJ_quVK0yR",
        "outputId": "1de88ccf-e0df-4ed7-94b3-bd58a020b84d"
      },
      "outputs": [],
      "source": [
        "\n",
        "id2word = {v:k for k,v in word2id.items()}\n",
        "# Decode the sentence\n",
        "decoded_sentence = [id2word.get(word_id, '<UNK>') for word_id in encoded_sentence]\n",
        "decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVJGY020K0yR",
        "outputId": "737b23c4-01f3-4c18-8b01-55e1193bbfc3"
      },
      "outputs": [],
      "source": [
        "## Convert to sentence with masked entity\n",
        "sentence_masked = []\n",
        "for index, lab in enumerate(label):\n",
        "    if lab == 'O':\n",
        "        sentence_masked.append(sentence[index])\n",
        "    else:\n",
        "        sentence_masked.append(label[index])\n",
        "sentence_masked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZI4K4AXK0yR",
        "outputId": "61b0e07d-5de3-4254-d08f-0049d6d1023e"
      },
      "outputs": [],
      "source": [
        "print(f\"X shape: {X.shape}, y shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrZ5bat5K0yR",
        "outputId": "a0e3cfb4-f14d-4856-effc-0c9191e8443f"
      },
      "outputs": [],
      "source": [
        "vocab_size,label_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "gn_ogqkvK0yR",
        "outputId": "4f795ff2-e101-4817-9b75-bcb08bcd8a15"
      },
      "outputs": [],
      "source": [
        "valid_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddc8fff0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the loss curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(loss_train, label='Training Loss')\n",
        "plt.plot(loss_valid, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting the accuracy curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(acc_train, label='Training Accuracy')\n",
        "plt.plot(acc_valid, label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting the F1-score curve\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(f1_valid, label='Validation F1-score')\n",
        "plt.title('Validation F1-score')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('F1-score')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "e6c8e7ba",
        "outputId": "ed0f8b2c-aeab-4aa7-d2d7-c8ecc0535a3b"
      },
      "outputs": [],
      "source": [
        "def eval_fn(model, test_dataloader):\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(test_dataloader):\n",
        "\n",
        "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['attn_masks'].to(device, dtype = torch.long)\n",
        "            targets = batch['labels'].to(device, dtype = torch.long)\n",
        "\n",
        "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "            loss, eval_logits = outputs.loss, outputs.logits\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += targets.size(0)\n",
        "\n",
        "            if idx % 40==0:\n",
        "                loss_step = eval_loss/nb_eval_steps\n",
        "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
        "\n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "\n",
        "            # now, use mask to determine where we should compare predictions with\n",
        "            # targets (includes [CLS] and [SEP] token predictions)\n",
        "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
        "\n",
        "            # actual targets\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy).cpu().numpy()\n",
        "            # actual preicted labels\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy).cpu().numpy()\n",
        "\n",
        "\n",
        "            real_tar_ = []\n",
        "            real_pred_ = []\n",
        "\n",
        "            for t,p in zip(targets, predictions):\n",
        "                if t != -100:\n",
        "                    real_tar_.append(t)\n",
        "                    real_pred_.append(p)\n",
        "\n",
        "            eval_labels.extend(real_tar_)\n",
        "            eval_preds.extend(real_pred_)\n",
        "\n",
        "            tmp_eval_accuracy = accuracy_score(real_tar_, real_pred_)\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return eval_labels, eval_preds\n",
        "# Evaluate the model on the test set\n",
        "true_labels, pred_labels = eval_fn(trained_model, test_dataloader)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"\\nEvaluation Metrics for Test DataSet\")\n",
        "print(\"=========================================\")\n",
        "print_full_classification_report(true_labels, pred_labels)\n",
        "\n",
        "# Print the label-wise classification report\n",
        "clf_report_for_test = classification_report(y_true=true_labels,\n",
        "                                             y_pred=pred_labels,\n",
        "                                             labels=[x for x in range(len(label2id))],\n",
        "                                             target_names=list(label2id.keys()),\n",
        "                                             zero_division=0\n",
        "                                            )\n",
        "print(\"\\nLabel-Wise Evaluation Metrics for Test DataSet\")\n",
        "print(\"Classification Report for each label\\n\")\n",
        "print(clf_report_for_test)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "000def3e314f42298493051ff0674214": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0508a08ac8184c02b6cebaac90ac1cd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "09a70faf28404e309f4e3c05f1bb02d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b339696365b4b688efe454f02607bbe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "189c69dfdfa2481ab24a59479ff5e97b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f7593918c5e401ab789a1771d909825": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f94123f46274d67ac421f2e062629c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0a04f86840542d8b84121ec76c12747",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b013535fc04445d84be0024e99f8510",
            "value": 570
          }
        },
        "272b2672cd684e15ba61975a6c2e4c9c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29f72c857f4a495ab411d01714a17c26": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_858ada749ff54f629903a17624aa90f4",
            "placeholder": "​",
            "style": "IPY_MODEL_d4c31856b7f2490eb99bdadae3321f08",
            "value": "model.safetensors: 100%"
          }
        },
        "2a192389e7bb4d5193142ffbc4d72b73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71da289247ab4c41a926843721a9bfe9",
            "placeholder": "​",
            "style": "IPY_MODEL_b1d1561ad9e74e5a973fbed659cb87b3",
            "value": " 232k/232k [00:00&lt;00:00, 16.5MB/s]"
          }
        },
        "2efbc36a66b346d7a8a69fca6e5f24e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_642267030b7e4e20842d57c075d55f49",
              "IPY_MODEL_864761d672c3454f9b836f2eea1caf8b",
              "IPY_MODEL_5f18b2cfafe844458e7ff0a443690cba"
            ],
            "layout": "IPY_MODEL_5bb7d1ef4be44d89b242119e853a61c3"
          }
        },
        "2f06215b6a2647af895382b203feaca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09a70faf28404e309f4e3c05f1bb02d8",
            "placeholder": "​",
            "style": "IPY_MODEL_1f7593918c5e401ab789a1771d909825",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "3813d23c01f14f3b95fce2ca712fd93a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65b8dd4c86d2493095b3386b8df7e7ea",
            "placeholder": "​",
            "style": "IPY_MODEL_b51e3e2196874dd68a98147e50aaf729",
            "value": " 48.0/48.0 [00:00&lt;00:00, 5.30kB/s]"
          }
        },
        "3a1b0290fdbd486c9a70310f95b9bf34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0ef094514414301aee9f1a1d8ca4fe4",
            "placeholder": "​",
            "style": "IPY_MODEL_dd73a726e8ec4e8fa5e39addf6c5af66",
            "value": "vocab.txt: 100%"
          }
        },
        "3f653388b32a4bfaade45629776172b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b339696365b4b688efe454f02607bbe",
            "placeholder": "​",
            "style": "IPY_MODEL_47defa877abe48809db99774d04cb8ba",
            "value": " 570/570 [00:00&lt;00:00, 76.9kB/s]"
          }
        },
        "3f91a93bb6614087bb8509981f32030a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e93efa09e6c4c3e9dae5322e3f881c4",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0508a08ac8184c02b6cebaac90ac1cd9",
            "value": 440449768
          }
        },
        "47defa877abe48809db99774d04cb8ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b013535fc04445d84be0024e99f8510": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "509afe3d572f490fac958ed4443aedbe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53f16e014cc84f9ab2c7a64f2eb507c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5945c8126e904191b6ca1312f361cff0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f06215b6a2647af895382b203feaca0",
              "IPY_MODEL_e1de02d8ac4b45e8b6c35dfc1e71dc90",
              "IPY_MODEL_3813d23c01f14f3b95fce2ca712fd93a"
            ],
            "layout": "IPY_MODEL_189c69dfdfa2481ab24a59479ff5e97b"
          }
        },
        "5bb7d1ef4be44d89b242119e853a61c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e93efa09e6c4c3e9dae5322e3f881c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f18b2cfafe844458e7ff0a443690cba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92f2e86512b4444d8f5fbd474367043a",
            "placeholder": "​",
            "style": "IPY_MODEL_c61e8d6ebe904485985037e888a2537b",
            "value": " 466k/466k [00:00&lt;00:00, 1.08MB/s]"
          }
        },
        "642267030b7e4e20842d57c075d55f49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa0398dc830b4455abf474e7248a20a4",
            "placeholder": "​",
            "style": "IPY_MODEL_dd6d233709414a12b34c11c6e56265ec",
            "value": "tokenizer.json: 100%"
          }
        },
        "65b8dd4c86d2493095b3386b8df7e7ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69ea32dff4484f5ca17225f500e805c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ee51519ddba4bfca279e62d7d8104d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fbcc6b31ff4498fa8edf86de7054534": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71da289247ab4c41a926843721a9bfe9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f6f39d5105b4e9eb7a239485fdf7fe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53f16e014cc84f9ab2c7a64f2eb507c2",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_000def3e314f42298493051ff0674214",
            "value": 231508
          }
        },
        "81d1553105a84128aa82671747901ca0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "858ada749ff54f629903a17624aa90f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "864761d672c3454f9b836f2eea1caf8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e76469bbe36c47ecab85dee37c612b07",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9981c20b74064340b1061117a706fec0",
            "value": 466062
          }
        },
        "8e6fe204977042b7b1875dc1285cff08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_29f72c857f4a495ab411d01714a17c26",
              "IPY_MODEL_3f91a93bb6614087bb8509981f32030a",
              "IPY_MODEL_d718abdd046d41b2995a2fe0be985f28"
            ],
            "layout": "IPY_MODEL_a00af3622f5e497da3c3a90d821483c7"
          }
        },
        "924fd220b99f463a9319d2db3e97b6da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81d1553105a84128aa82671747901ca0",
            "placeholder": "​",
            "style": "IPY_MODEL_69ea32dff4484f5ca17225f500e805c9",
            "value": "config.json: 100%"
          }
        },
        "92f2e86512b4444d8f5fbd474367043a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9981c20b74064340b1061117a706fec0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a00af3622f5e497da3c3a90d821483c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa0398dc830b4455abf474e7248a20a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1d1561ad9e74e5a973fbed659cb87b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b51e3e2196874dd68a98147e50aaf729": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba99a996219b4ef8a9b2cc5430f914be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a1b0290fdbd486c9a70310f95b9bf34",
              "IPY_MODEL_7f6f39d5105b4e9eb7a239485fdf7fe9",
              "IPY_MODEL_2a192389e7bb4d5193142ffbc4d72b73"
            ],
            "layout": "IPY_MODEL_272b2672cd684e15ba61975a6c2e4c9c"
          }
        },
        "c61e8d6ebe904485985037e888a2537b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4c31856b7f2490eb99bdadae3321f08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d718abdd046d41b2995a2fe0be985f28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ee51519ddba4bfca279e62d7d8104d8",
            "placeholder": "​",
            "style": "IPY_MODEL_6fbcc6b31ff4498fa8edf86de7054534",
            "value": " 440M/440M [00:01&lt;00:00, 381MB/s]"
          }
        },
        "dd6d233709414a12b34c11c6e56265ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd73a726e8ec4e8fa5e39addf6c5af66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e05581d49f71477f84f40892b5c8a745": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0a04f86840542d8b84121ec76c12747": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1de02d8ac4b45e8b6c35dfc1e71dc90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e05581d49f71477f84f40892b5c8a745",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4c255c77579465797492281917c352c",
            "value": 48
          }
        },
        "e4c255c77579465797492281917c352c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e76469bbe36c47ecab85dee37c612b07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0ef094514414301aee9f1a1d8ca4fe4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fef7ee92f3c94dac97c1eb4434a32e8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_924fd220b99f463a9319d2db3e97b6da",
              "IPY_MODEL_1f94123f46274d67ac421f2e062629c0",
              "IPY_MODEL_3f653388b32a4bfaade45629776172b4"
            ],
            "layout": "IPY_MODEL_509afe3d572f490fac958ed4443aedbe"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
